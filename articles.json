[{"model": "App.cryptographyarticle", "pk": 1, "fields": {"title": "Efficient Ranking, Order Statistics, and Sorting under CKKS", "date": "2024-12-19", "authors": "Federico Mazzone, Maarten Everts, Florian Hahn, Andreas Peter", "content": "Fully Homomorphic Encryption (FHE) enables operations on encrypted data,\nmaking it extremely useful for privacy-preserving applications, especially in\ncloud computing environments. In such contexts, operations like ranking, order\nstatistics, and sorting are fundamental functionalities often required for\ndatabase queries or as building blocks of larger protocols. However, the high\ncomputational overhead and limited native operations of FHE pose significant\nchallenges for an efficient implementation of these tasks. These challenges are\nexacerbated by the fact that all these functionalities are based on comparing\nelements, which is a severely expensive operation under encryption.\n  Previous solutions have typically based their designs on swap-based\ntechniques, where two elements are conditionally swapped based on the results\nof their comparison. These methods aim to reduce the primary computational\nbottleneck: the comparison depth, which is the number of non-parallelizable\nhomomorphic comparisons. The current state of the art solution for sorting by\nLu et al. (IEEE S&P'21), for instance, achieves a comparison depth of\nO(log^2(N)).\n  In this paper, we address the challenge of reducing the comparison depth by\nshifting away from the swap-based paradigm. We present solutions for ranking,\norder statistics, and sorting, that all achieve a comparison depth of O(1),\nmaking our approach highly parallelizable. Leveraging the SIMD capabilities of\nthe CKKS FHE scheme, our approach re-encodes the input vector under encryption\nto allow for simultaneous comparisons of all elements with each other. The\nhomomorphic re-encoding incurs a minimal computational overhead of O(log(N))\nrotations. Experimental results show that our approach ranks a 128-element\nvector in approximately 2.64s, computes its argmin/argmax in 14.18s, and sorts\nit in 21.10s.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.15126v1"}}, {"model": "App.cryptographyarticle", "pk": 2, "fields": {"title": "ScamChatBot: An End-to-End Analysis of Fake Account Recovery on Social Media via Chatbots", "date": "2024-12-19", "authors": "Bhupendra Acharya, Dominik Sautter, Muhammad Saad, Thorsten Holz", "content": "Social media platforms have become the hubs for various user interactions\ncovering a wide range of needs, including technical support and services\nrelated to brands, products, or user accounts. Unfortunately, there has been a\nrecent surge in scammers impersonating official services and providing fake\ntechnical support to users through these platforms. In this study, we focus on\nscammers engaging in such fake technical support to target users who are having\nproblems recovering their accounts. More specifically, we focus on users\nencountering access problems with social media profiles (e.g., on platforms\nsuch as Facebook, Instagram, Gmail, and X) and cryptocurrency wallets. The main\ncontribution of our work is the development of an automated system that\ninteracts with scammers via a chatbot that mimics different personas. By\ninitiating decoy interactions (e.g., through deceptive tweets), we have enticed\nscammers to interact with our system so that we can analyze their modus\noperandi. Our results show that scammers employ many social media profiles\nasking users to contact them via a few communication channels. Using a large\nlanguage model (LLM), our chatbot had conversations with 450 scammers and\nprovided valuable insights into their tactics and, most importantly, their\npayment profiles. This automated approach highlights how scammers use a variety\nof strategies, including role-playing, to trick victims into disclosing\npersonal or financial information. With this study, we lay the foundation for\nusing automated chat-based interactions with scammers to detect and study\nfraudulent activities at scale in an automated way.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.15072v1"}}, {"model": "App.cryptographyarticle", "pk": 3, "fields": {"title": "Large Language Models and Code Security: A Systematic Literature Review", "date": "2024-12-19", "authors": "Enna Basic, Alberto Giaretta", "content": "Large Language Models (LLMs) have emerged as powerful tools for automating\nvarious programming tasks, including security-related ones, such as detecting\nand fixing vulnerabilities. Despite their promising capabilities, when required\nto produce or modify pre-existing code, LLMs could introduce vulnerabilities\nunbeknown to the programmer. When analyzing code, they could miss clear\nvulnerabilities or signal nonexistent ones. In this Systematic Literature\nReview (SLR), we aim to investigate both the security benefits and potential\ndrawbacks of using LLMs for a variety of code-related tasks. In particular,\nfirst we focus on the types of vulnerabilities that could be introduced by\nLLMs, when used for producing code. Second, we analyze the capabilities of LLMs\nto detect and fix vulnerabilities, in any given code, and how the prompting\nstrategy of choice impacts their performance in these two tasks. Last, we\nprovide an in-depth analysis on how data poisoning attacks on LLMs can impact\nperformance in the aforementioned tasks.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.15004v1"}}, {"model": "App.cryptographyarticle", "pk": 4, "fields": {"title": "Single-Photon Advantage in Quantum Cryptography Beyond QKD", "date": "2024-12-19", "authors": "Daniel A. Vajner, Koray Kaymazlar, Fenja Drauschke, Lucas Rickert, Martin von Helversen, Hanqing Liu, Shulun Li, Haiqiao Ni, Zhichuan Niu, Anna Pappa, Tobias Heindel", "content": "In quantum cryptography, fundamental laws of quantum physics are exploited to\nenhance the security of cryptographic tasks. Quantum key distribution is by far\nthe most studied protocol to date, enabling the establishment of a secret key\nbetween trusted parties. However, there exist many practical use-cases in\ncommunication networks, which also involve parties in distrustful settings. The\nmost fundamental quantum cryptographic building block in such a distrustful\nsetting is quantum coin flipping, which provides an advantage compared to its\nclassical equivalent. So far, few experimental studies on quantum coin flipping\nhave been reported, all of which used probabilistic quantum light sources\nfacing fundamental limitations. Here, we experimentally implement a quantum\nstrong coin flipping protocol using single-photon states and demonstrate an\nadvantage compared to both classical realizations and implementations using\nfaint laser pulses. We achieve this by employing a state-of-the-art\ndeterministic single-photon source based on the Purcell-enhanced emission of a\nsemiconductor quantum dot in combination with fast polarization-state encoding\nenabling a quantum bit error ratio below 3%, required for the successful\nexecution of the protocol. The reduced multi-photon emission yields a smaller\nbias of the coin flipping protocol compared to an attenuated laser\nimplementation, both in simulations and in the experiment. By demonstrating a\nsingle-photon quantum advantage in a cryptographic primitive beyond QKD, our\nwork represents a major advance towards the implementation of complex\ncryptographic tasks in a future quantum internet.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14993v1"}}, {"model": "App.cryptographyarticle", "pk": 5, "fields": {"title": "Exploration of the Dynamics of Buy and Sale of Social Media Accounts", "date": "2024-12-19", "authors": "Mario Beluri, Bhupendra Acharya, Soheil Khodayari, Giada Stivala, Giancarlo Pellegrino, Thorsten Holz", "content": "There has been a rise in online platforms facilitating the buying and selling\nof social media accounts. While the trade of social media profiles is not\ninherently illegal, social media platforms view such transactions as violations\nof their policies. They often take action against accounts involved in the\nmisuse of platforms for financial gain. This research conducts a comprehensive\nanalysis of marketplaces that enable the buying and selling of social media\naccounts.\n  We investigate the economic scale of account trading across five major\nplatforms: X, Instagram, Facebook, TikTok, and YouTube. From February to June\n2024, we identified 38,253 accounts advertising account sales across 11 online\nmarketplaces, covering 211 distinct categories. The total value of marketed\nsocial media accounts exceeded \\$64 million, with a median price of \\$157 per\naccount. Additionally, we analyzed the profiles of 11,457 visible advertised\naccounts, collecting their metadata and over 200,000 profile posts. By\nexamining their engagement patterns and account creation methods, we evaluated\nthe fraudulent activities commonly associated with these sold accounts. Our\nresearch reveals these marketplaces foster fraudulent activities such as bot\nfarming, harvesting accounts for future fraud, and fraudulent engagement. Such\npractices pose significant risks to social media users, who are often targeted\nby fraudulent accounts resembling legitimate profiles and employing social\nengineering tactics. We highlight social media platform weaknesses in the\nability to detect and mitigate such fraudulent accounts, thereby endangering\nusers. Alongside this, we conducted thorough disclosures with the respective\nplatforms and proposed actionable recommendations, including indicators to\nidentify and track these accounts. These measures aim to enhance proactive\ndetection and safeguard users from potential threats.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14985v1"}}, {"model": "App.cryptographyarticle", "pk": 6, "fields": {"title": "Position: A taxonomy for reporting and describing AI security incidents", "date": "2024-12-19", "authors": "Lukas Bieringer, Kevin Paeth, Andreas Wespi, Kathrin Grosse", "content": "AI systems are vulnerable to attacks, and corresponding AI security incidents\nhave been described. Although a collection of safety incidents around AI will\nbecome a regulatory requirement, there is no proposal to collect AI security\nincidents. In this position paper, we argue that a proposal should be made,\ntaking into account the interests and needs of different stakeholders:\nindustry, providers, users, and researchers. We thus attempt to close this gap\nand propose a taxonomy alongside its requirements like machine readability and\nlink-ability with existing databases. We aim to spark discussions and enable\ndiscussion of which information is feasible, necessary, and possible to report\nand share within and outside organizations using AI.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14855v1"}}, {"model": "App.cryptographyarticle", "pk": 7, "fields": {"title": "Federated Heavy Hitter Analytics with Local Differential Privacy", "date": "2024-12-19", "authors": "Yuemin Zhang, Qingqing Ye, Haibo Hu", "content": "Federated heavy hitter analytics enables service providers to better\nunderstand the preferences of cross-party users by analyzing the most frequent\nitems. As with federated learning, it faces challenges of privacy concerns,\nstatistical heterogeneity, and expensive communication. Local differential\nprivacy (LDP), as the \\textit{de facto} standard for privacy-preserving data\ncollection, solves the privacy challenge by letting each user perturb her data\nlocally and report the sanitized version. However, in federated settings,\napplying LDP complicates the other two challenges, due to the deteriorated\nutility by the injected LDP noise or increasing communication/computation costs\nby perturbation mechanism. To tackle these problems, we propose a novel\ntarget-aligning prefix tree mechanism satisfying $\\epsilon$-LDP, for federated\nheavy hitter analytics. In particular, we propose an adaptive extension\nstrategy to address the inconsistencies between covering necessary prefixes and\nestimating heavy hitters within a party to enhance the utility. We also present\na consensus-based pruning strategy that utilizes noisy prior knowledge from\nother parties to further align the inconsistency between finding heavy hitters\nin each party and providing reasonable frequency information to identify the\nglobal ones. To the best of our knowledge, our study is the first solution to\nthe federated heavy hitter analytics in a cross-party setting while satisfying\nthe stringent $\\epsilon$-LDP. Comprehensive experiments on both real-world and\nsynthetic datasets confirm the effectiveness of our proposed mechanism.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14832v1"}}, {"model": "App.cryptographyarticle", "pk": 8, "fields": {"title": "Non-intrusive and Unconstrained Keystroke Inference in VR Platforms via Infrared Side Channel", "date": "2024-12-19", "authors": "Tao Ni, Yuefeng Du, Qingchuan Zhao, Cong Wang", "content": "Virtual Reality (VR) technologies are increasingly employed in numerous\napplications across various areas. Therefore, it is essential to ensure the\nsecurity of interactions between users and VR devices. In this paper, we\ndisclose a new side-channel leakage in the constellation tracking system of\nmainstream VR platforms, where the infrared (IR) signals emitted from the VR\ncontrollers for controller-headset interactions can be maliciously exploited to\nreconstruct unconstrained input keystrokes on the virtual keyboard\nnon-intrusively. We propose a novel keystroke inference attack named VRecKey to\ndemonstrate the feasibility and practicality of this novel infrared side\nchannel. Specifically, VRecKey leverages a customized 2D IR sensor array to\nintercept ambient IR signals emitted from VR controllers and subsequently\ninfers (i) character-level key presses on the virtual keyboard and (ii)\nword-level keystrokes along with their typing trajectories. We extensively\nevaluate the effectiveness of VRecKey with two commercial VR devices, and the\nresults indicate that it can achieve over 94.2% and 90.5% top-3 accuracy in\ninferring character-level and word-level keystrokes with varying lengths,\nrespectively. In addition, empirical results show that VRecKey is resilient to\nseveral practical impact factors and presents effectiveness in various\nreal-world scenarios, which provides a complementary and orthogonal attack\nsurface for the exploration of keystroke inference attacks in VR platforms.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14815v1"}}, {"model": "App.cryptographyarticle", "pk": 9, "fields": {"title": "Gaussian boson sampling for binary optimization", "date": "2024-12-19", "authors": "Jean Cazalis, Tirth Shah, Yahui Chai, Karl Jansen, Stefan Kühn", "content": "Binary optimization is a fundamental area in computational science, with\nwide-ranging applications from logistics to cryptography, where the tasks are\noften formulated as Quadratic or Polynomial Unconstrained Binary Optimization\nproblems (QUBO/PUBO). In this work, we propose to use a parametrized Gaussian\nBoson Sampler (GBS) with threshold detectors to address such problems. We map\ngeneral PUBO instance onto a quantum Hamiltonian and optimize the Conditional\nValue-at-Risk of its energy with respect to the GBS ansatz. In particular, we\nobserve that, when the algorithm reduces to standard Variational Quantum\nEigensolver, the cost function is analytical. Therefore, it can be computed\nefficiently, along with its gradient, for low-degree polynomials using only\nclassical computing resources. Numerical experiments on 3-SAT and Graph\nPartitioning problems show significant performance gains over random guessing,\nproviding a first proof of concept for our proposed approach.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14783v1"}}, {"model": "App.cryptographyarticle", "pk": 10, "fields": {"title": "Simplicity over Complexity: An ARN-Based Intrusion Detection Method for Industrial Control Network", "date": "2024-12-19", "authors": "Ziyi Liu, Dengpan Ye, Changsong Yang, Yong Ding, Yueling Liu, Long Tang, Chuanxi Chen", "content": "Industrial control network (ICN) is characterized by real-time responsiveness\nand reliability, which plays a key role in increasing production speed,\nrational and efficient processing, and managing the production process. Despite\ntremendous advantages, ICN inevitably struggles with some challenges, such as\nmalicious user intrusion and hacker attack. To detect malicious intrusions in\nICN, intrusion detection systems have been deployed. However, in ICN, network\ntraffic data is equipped with characteristics of large scale, irregularity,\nmultiple features, temporal correlation and high dimensionality, which greatly\naffect the efficiency and performance. To properly solve the above problems, we\ndesign a new intrusion detection method for ICN. Specifically, we first design\na novel neural network model called associative recurrent network (ARN), which\ncan properly handle the relationship between past moment hidden state and\ncurrent moment information. Then, we adopt ARN to design a new intrusion\ndetection method that can efficiently and accurately detect malicious\nintrusions in ICN. Subsequently, we demonstrate the high efficiency of our\nproposed method through theoretical computational complexity analysis. Finally,\nwe develop a prototype implementation to evaluate the accuracy. The\nexperimental results prove that our proposed method has sate-of-the-art\nperformance on both the ICN dataset SWaT and the conventional network traffic\ndataset UNSW-NB15. The accuracies on the SWaT dataset and the UNSW-NB15 dataset\nreach 95.48% and 97.61%, respectively.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14669v1"}}, {"model": "App.cryptographyarticle", "pk": 11, "fields": {"title": "A Shapley Value Estimation Speedup for Efficient Explainable Quantum AI", "date": "2024-12-19", "authors": "Iain Burge, Michel Barbeau, Joaquin Garcia-Alfaro", "content": "This work focuses on developing efficient post-hoc explanations for quantum\nAI algorithms. In classical contexts, the cooperative game theory concept of\nthe Shapley value adapts naturally to post-hoc explanations, where it can be\nused to identify which factors are important in an AI's decision-making\nprocess. An interesting question is how to translate Shapley values to the\nquantum setting and whether quantum effects could be used to accelerate their\ncalculation. We propose quantum algorithms that can extract Shapley values\nwithin some confidence interval. Our method is capable of quadratically\noutperforming classical Monte Carlo approaches to approximating Shapley values\nup to polylogarithmic factors in various circumstances. We demonstrate the\nvalidity of our approach empirically with specific voting games and provide\nrigorous proofs of performance for general cooperative games.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14639v1"}}, {"model": "App.cryptographyarticle", "pk": 12, "fields": {"title": "FRIDAY: Mitigating Unintentional Facial Identity in Deepfake Detectors Guided by Facial Recognizers", "date": "2024-12-19", "authors": "Younhun Kim, Myung-Joon Kwon, Wonjun Lee, Changick Kim", "content": "Previous Deepfake detection methods perform well within their training\ndomains, but their effectiveness diminishes significantly with new synthesis\ntechniques. Recent studies have revealed that detection models often create\ndecision boundaries based on facial identity rather than synthetic artifacts,\nresulting in poor performance on cross-domain datasets. To address this\nlimitation, we propose Facial Recognition Identity Attenuation (FRIDAY), a\nnovel training method that mitigates facial identity influence using a face\nrecognizer. Specifically, we first train a face recognizer using the same\nbackbone as the Deepfake detector. The recognizer is then frozen and employed\nduring the detector's training to reduce facial identity information. This is\nachieved by feeding input images into both the recognizer and the detector, and\nminimizing the similarity of their feature embeddings through our Facial\nIdentity Attenuating loss. This process encourages the detector to generate\nembeddings distinct from the recognizer, effectively reducing the impact of\nfacial identity. Extensive experiments demonstrate that our approach\nsignificantly enhances detection performance on both in-domain and cross-domain\ndatasets.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14623v1"}}, {"model": "App.cryptographyarticle", "pk": 13, "fields": {"title": "AIArena: A Blockchain-Based Decentralized AI Training Platform", "date": "2024-12-19", "authors": "Zhipeng Wang, Rui Sun, Elizabeth Lui, Tuo Zhou, Yizhe Wen, Jiahao Sun", "content": "The rapid advancement of AI has underscored critical challenges in its\ndevelopment and implementation, largely due to centralized control by a few\nmajor corporations. This concentration of power intensifies biases within AI\nmodels, resulting from inadequate governance and oversight mechanisms.\nAdditionally, it limits public involvement and heightens concerns about the\nintegrity of model generation. Such monopolistic control over data and AI\noutputs threatens both innovation and fair data usage, as users inadvertently\ncontribute data that primarily benefits these corporations. In this work, we\npropose AIArena, a blockchain-based decentralized AI training platform designed\nto democratize AI development and alignment through on-chain incentive\nmechanisms. AIArena fosters an open and collaborative environment where\nparticipants can contribute models and computing resources. Its on-chain\nconsensus mechanism ensures fair rewards for participants based on their\ncontributions. We instantiate and implement AIArena on the public Base\nblockchain Sepolia testnet, and the evaluation results demonstrate the\nfeasibility of AIArena in real-world applications.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14566v1"}}, {"model": "App.cryptographyarticle", "pk": 14, "fields": {"title": "Guided Diffusion Model for Sensor Data Obfuscation", "date": "2024-12-19", "authors": "Xin Yang, Omid Ardakanian", "content": "Sensor data collected by Internet of Things (IoT) devices carries detailed\ninformation about individuals in their vicinity. Sharing this data with a\nsemi-trusted service provider may compromise the individuals' privacy, as\nsensitive information can be extracted by powerful machine learning models.\nData obfuscation empowered by generative models is a promising approach to\ngenerate synthetic sensor data such that the useful information contained in\nthe original data is preserved and the sensitive information is obscured. This\nnewly generated data will then be shared with the service provider instead of\nthe original sensor data. In this work, we propose PrivDiffuser, a novel data\nobfuscation technique based on a denoising diffusion model that attains a\nsuperior trade-off between data utility and privacy through effective guidance\ntechniques. Specifically, we extract latent representations that contain\ninformation about public and private attributes from sensor data to guide the\ndiffusion model, and impose mutual information-based regularization when\nlearning the latent representations to alleviate the entanglement of public and\nprivate attributes, thereby increasing the effectiveness of guidance.\nEvaluation on three real-world datasets containing different sensing modalities\nreveals that PrivDiffuser yields a better privacy-utility trade-off than the\nstate-of-the-art obfuscation model, decreasing the utility loss by up to\n$1.81\\%$ and the privacy loss by up to $3.42\\%$. Moreover, we showed that users\nwith diverse privacy needs can use PrivDiffuser to protect their privacy\nwithout having to retrain the model.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14499v1"}}, {"model": "App.cryptographyarticle", "pk": 15, "fields": {"title": "FedMUP: Federated Learning driven Malicious User Prediction Model for Secure Data Distribution in Cloud Environments", "date": "2024-12-19", "authors": "Kishu Gupta, Deepika Saxena, Rishabh Gupta, Jatinder Kumar, Ashutosh Kumar Singh", "content": "Cloud computing is flourishing at a rapid pace. Significant consequences\nrelated to data security appear as a malicious user may get unauthorized access\nto sensitive data which may be misused, further. This raises an alarm-ringing\nsituation to tackle the crucial issue related to data security and proactive\nmalicious user prediction. This article proposes a Federated learning driven\nMalicious User Prediction Model for Secure Data Distribution in Cloud\nEnvironments (FedMUP). This approach firstly analyses user behavior to acquire\nmultiple security risk parameters. Afterward, it employs the federated\nlearning-driven malicious user prediction approach to reveal doubtful users,\nproactively. FedMUP trains the local model on their local dataset and transfers\ncomputed values rather than actual raw data to obtain an updated global model\nbased on averaging various local versions. This updated model is shared\nrepeatedly at regular intervals with the user for retraining to acquire a\nbetter, and more efficient model capable of predicting malicious users more\nprecisely. Extensive experimental work and comparison of the proposed model\nwith state-of-the-art approaches demonstrate the efficiency of the proposed\nwork. Significant improvement is observed in the key performance indicators\nsuch as malicious user prediction accuracy, precision, recall, and f1-score up\nto 14.32%, 17.88%, 14.32%, and 18.35%, respectively.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14495v1"}}, {"model": "App.cryptographyarticle", "pk": 16, "fields": {"title": "MAIDS: Malicious Agent Identification-based Data Security Model for Cloud Environments", "date": "2024-12-19", "authors": "Kishu Gupta, Deepika Saxena, Rishabh Gupta, Ashutosh Kumar Singh", "content": "With the vigorous development of cloud computing, most organizations have\nshifted their data and applications to the cloud environment for storage,\ncomputation, and sharing purposes. During storage and data sharing across the\nparticipating entities, a malicious agent may gain access to outsourced data\nfrom the cloud environment. A malicious agent is an entity that deliberately\nbreaches the data. This information accessed might be misused or revealed to\nunauthorized parties. Therefore, data protection and prediction of malicious\nagents have become a demanding task that needs to be addressed appropriately.\nTo deal with this crucial and challenging issue, this paper presents a\nMalicious Agent Identification-based Data Security (MAIDS) Model which utilizes\nXGBoost machine learning classification algorithm for securing data allocation\nand communication among different participating entities in the cloud system.\nThe proposed model explores and computes intended multiple security parameters\nassociated with online data communication or transactions. Correspondingly, a\nsecurity-focused knowledge database is produced for developing the XGBoost\nClassifier-based Malicious Agent Prediction (XC-MAP) unit. Unlike the existing\napproaches, which only identify malicious agents after data leaks, MAIDS\nproactively identifies malicious agents by examining their eligibility for\nrespective data access. In this way, the model provides a comprehensive\nsolution to safeguard crucial data from both intentional and non-intentional\nbreaches, by granting data to authorized agents only by evaluating the agents\nbehavior and predicting the malicious agent before granting data.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14490v1"}}, {"model": "App.cryptographyarticle", "pk": 17, "fields": {"title": "Towards Provable Security in Industrial Control Systems Via Dynamic Protocol Attestation", "date": "2024-12-19", "authors": "Arthur Amorim, Trevor Kann, Max Taylor, Lance Joneckis", "content": "Industrial control systems (ICSs) increasingly rely on digital technologies\nvulnerable to cyber attacks. Cyber attackers can infiltrate ICSs and execute\nmalicious actions. Individually, each action seems innocuous. But taken\ntogether, they cause the system to enter an unsafe state. These attacks have\nresulted in dramatic consequences such as physical damage, economic loss, and\nenvironmental catastrophes. This paper introduces a methodology that restricts\nactions using protocols. These protocols only allow safe actions to execute.\nProtocols are written in a domain specific language we have embedded in an\ninteractive theorem prover (ITP). The ITP enables formal, machine-checked\nproofs to ensure protocols maintain safety properties. We use dynamic\nattestation to ensure ICSs conform to their protocol even if an adversary\ncompromises a component. Since protocol conformance prevents unsafe actions,\nthe previously mentioned cyber attacks become impossible. We demonstrate the\neffectiveness of our methodology using an example from the Fischertechnik\nIndustry 4.0 platform. We measure dynamic attestation's impact on latency and\nthroughput. Our approach is a starting point for studying how to combine formal\nmethods and protocol design to thwart attacks intended to cripple ICSs.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14467v1"}}, {"model": "App.cryptographyarticle", "pk": 18, "fields": {"title": "Fingerprinting Codes Meet Geometry: Improved Lower Bounds for Private Query Release and Adaptive Data Analysis", "date": "2024-12-18", "authors": "Xin Lyu, Kunal Talwar", "content": "Fingerprinting codes are a crucial tool for proving lower bounds in\ndifferential privacy. They have been used to prove tight lower bounds for\nseveral fundamental questions, especially in the ``low accuracy'' regime.\nUnlike reconstruction/discrepancy approaches however, they are more suited for\nquery sets that arise naturally from the fingerprinting codes construction. In\nthis work, we propose a general framework for proving fingerprinting type lower\nbounds, that allows us to tailor the technique to the geometry of the query\nset. Our approach allows us to prove several new results, including the\nfollowing.\n  First, we show that any (sample- and population-)accurate algorithm for\nanswering $Q$ arbitrary adaptive counting queries over a universe $\\mathcal{X}$\nto accuracy $\\alpha$ needs $\\Omega(\\frac{\\sqrt{\\log |\\mathcal{X}|}\\cdot \\log\nQ}{\\alpha^3})$ samples, matching known upper bounds. This shows that the\napproaches based on differential privacy are optimal for this question, and\nimproves significantly on the previously known lower bounds of $\\frac{\\log\nQ}{\\alpha^2}$ and $\\min(\\sqrt{Q}, \\sqrt{\\log |\\mathcal{X}|})/\\alpha^2$.\n  Second, we show that any $(\\varepsilon,\\delta)$-DP algorithm for answering\n$Q$ counting queries to accuracy $\\alpha$ needs $\\Omega(\\frac{\\sqrt{\n\\log|\\mathcal{X}| \\log(1/\\delta)} \\log Q}{\\varepsilon\\alpha^2})$ samples,\nmatching known upper bounds up to constants. Our framework allows for proving\nthis bound via a direct correlation analysis and improves the prior bound of\n[BUV'14] by $\\sqrt{\\log(1/\\delta)}$.\n  Third, we characterize the sample complexity of answering a set of random\n$0$-$1$ queries under approximate differential privacy. We give new upper and\nlower bounds in different regimes. By combining them with known results, we can\ncomplete the whole picture.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14396v1"}}, {"model": "App.cryptographyarticle", "pk": 19, "fields": {"title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure Integration in Machine Learning Systems", "date": "2024-12-18", "authors": "Dongfang Zhao", "content": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14392v1"}}, {"model": "App.cryptographyarticle", "pk": 20, "fields": {"title": "Differentially Private Multi-objective Selection: Pareto and Aggregation Approaches", "date": "2024-12-18", "authors": "Victor A. E. Farias, Felipe T. Brito, Cheryl Flynn, Javam C. Machado, Divesh Srivastava", "content": "Differentially private selection mechanisms are fundamental building blocks\nfor privacy-preserving data analysis. While numerous mechanisms exist for\nsingle-objective selection, many real-world applications require optimizing\nmultiple competing objectives simultaneously. We present two novel mechanisms\nfor differentially private multi-objective selection: PrivPareto and PrivAgg.\nPrivPareto uses a novel Pareto score to identify solutions near the Pareto\nfrontier, while PrivAgg enables privacy-preserving weighted aggregation of\nmultiple objectives. Both mechanisms support global and local sensitivity\napproaches, with comprehensive theoretical analysis showing how to compose\nsensitivities of multiple utility functions. We demonstrate the practical\napplicability through two real-world applications: cost-sensitive decision tree\nconstruction and multi-objective influential node selection in social networks.\nThe experimental results showed that our local sensitivity-based approaches\nachieve significantly better utility compared to global sensitivity approaches\nacross both applications and both Pareto and Aggregation approaches. Moreover,\nthe local sensitivity-based approaches are able to perform well with typical\nprivacy budget values $\\epsilon \\in [0.01, 1]$ in most experiments.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14380v1"}}, {"model": "App.cryptographyarticle", "pk": 21, "fields": {"title": "Multi-user QKD using quotient graph states derived from continuous-variable dual-rail cluster states", "date": "2024-12-18", "authors": "Akash nag Oruganti", "content": "Multipartite entangled states are fundamental resources for multi-user\nquantum cryptographic tasks. Despite significant advancements in generating\nlarge-scale continuous-variable (CV) cluster states, particularly the dual-rail\ncluster state because of its utility in measurement-based quantum computation,\nits application in quantum cryptography has remained largely unexplored. In\nthis paper, we introduce a novel protocol for generating three user conference\nkeys using a CV dual-rail cluster state. We develop the concept of a quotient\ngraph state by applying a node coloring scheme to the infinite dual-rail graph,\nresulting in a six-mode pure graph state suitable for cryptographic\napplications. Our results demonstrate that the proposed protocol achieves\nperformance close to that of GHZ-based protocols for quantum conference key\nagreement (QCKA), with GHZ states performing slightly better. However, a key\nadvantage of our protocol lies in its ability to generate bipartite keys\npost-QCKA, a feature not achievable with GHZ states. Additionally, compared to\na downstream access network using two-mode squeezed vacuum states, our protocol\nachieves superior performance in generating bipartite keys. Furthermore, we\nextend our analysis to the finite-size regime and consider the impact of using\nimpure squeezed states for generating the multipartite entangled states,\nreflecting experimental imperfections. Our findings indicate that even with\nfinite resources and non-ideal state preparation, the proposed protocol\nmaintains its advantages. We also introduce a more accurate method to estimate\nthe capacity of a protocol to generate bipartite keys in a quantum network.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14317v1"}}, {"model": "App.cryptographyarticle", "pk": 22, "fields": {"title": "Closing the Gap: A User Study on the Real-world Usefulness of AI-powered Vulnerability Detection & Repair in the IDE", "date": "2024-12-18", "authors": "Benjamin Steenhoek, Kalpathy Sivaraman, Renata Saldivar Gonzalez, Yevhen Mohylevskyy, Roshanak Zilouchian Moghaddam, Wei Le", "content": "This paper presents the first empirical study of a vulnerability detection\nand fix tool with professional software developers on real projects that they\nown. We implemented DeepVulGuard, an IDE-integrated tool based on\nstate-of-the-art detection and fix models, and show that it has promising\nperformance on benchmarks of historic vulnerability data. DeepVulGuard scans\ncode for vulnerabilities (including identifying the vulnerability type and\nvulnerable region of code), suggests fixes, provides natural-language\nexplanations for alerts and fixes, leveraging chat interfaces. We recruited 17\nprofessional software developers at Microsoft, observed their usage of the tool\non their code, and conducted interviews to assess the tool's usefulness, speed,\ntrust, relevance, and workflow integration. We also gathered detailed\nqualitative feedback on users' perceptions and their desired features. Study\nparticipants scanned a total of 24 projects, 6.9k files, and over 1.7 million\nlines of source code, and generated 170 alerts and 50 fix suggestions. We find\nthat although state-of-the-art AI-powered detection and fix tools show promise,\nthey are not yet practical for real-world use due to a high rate of false\npositives and non-applicable fixes. User feedback reveals several actionable\npain points, ranging from incomplete context to lack of customization for the\nuser's codebase. Additionally, we explore how AI features, including confidence\nscores, explanations, and chat interaction, can apply to vulnerability\ndetection and fixing. Based on these insights, we offer practical\nrecommendations for evaluating and deploying AI detection and fix models. Our\ncode and data are available at https://doi.org/10.6084/m9.figshare.26367139.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14306v1"}}, {"model": "App.cryptographyarticle", "pk": 23, "fields": {"title": "Adversarial Hubness in Multi-Modal Retrieval", "date": "2024-12-18", "authors": "Tingwei Zhang, Fnu Suya, Rishi Jha, Collin Zhang, Vitaly Shmatikov", "content": "Hubness is a phenomenon in high-dimensional vector spaces where a single\npoint from the natural distribution is unusually close to many other points.\nThis is a well-known problem in information retrieval that causes some items to\naccidentally (and incorrectly) appear relevant to many queries. In this paper,\nwe investigate how attackers can exploit hubness to turn any image or audio\ninput in a multi-modal retrieval system into an adversarial hub. Adversarial\nhubs can be used to inject universal adversarial content (e.g., spam) that will\nbe retrieved in response to thousands of different queries, as well as for\ntargeted attacks on queries related to specific, attacker-chosen concepts. We\npresent a method for creating adversarial hubs and evaluate the resulting hubs\non benchmark multi-modal retrieval datasets and an image-to-image retrieval\nsystem based on a tutorial from Pinecone, a popular vector database. For\nexample, in text-caption-to-image retrieval, a single adversarial hub is\nretrieved as the top-1 most relevant image for more than 21,000 out of 25,000\ntest queries (by contrast, the most common natural hub is the top-1 response to\nonly 102 queries). We also investigate whether techniques for mitigating\nnatural hubness are an effective defense against adversarial hubs, and show\nthat they are not effective against hubs that target queries related to\nspecific concepts.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14113v1"}}, {"model": "App.cryptographyarticle", "pk": 24, "fields": {"title": "On the Robustness of Distributed Machine Learning against Transfer Attacks", "date": "2024-12-18", "authors": "Sébastien Andreina, Pascal Zimmer, Ghassan Karame", "content": "Although distributed machine learning (distributed ML) is gaining\nconsiderable attention in the community, prior works have independently looked\nat instances of distributed ML in either the training or the inference phase.\nNo prior work has examined the combined robustness stemming from distributing\nboth the learning and the inference process. In this work, we explore, for the\nfirst time, the robustness of distributed ML models that are fully\nheterogeneous in training data, architecture, scheduler, optimizer, and other\nmodel parameters. Supported by theory and extensive experimental validation\nusing CIFAR10 and FashionMNIST, we show that such properly distributed ML\ninstantiations achieve across-the-board improvements in accuracy-robustness\ntradeoffs against state-of-the-art transfer-based attacks that could otherwise\nnot be realized by current ensemble or federated learning instantiations. For\ninstance, our experiments on CIFAR10 show that for the Common Weakness attack,\none of the most powerful state-of-the-art transfer-based attacks, our method\nimproves robust accuracy by up to 40%, with a minimal impact on clean task\naccuracy.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14080v1"}}, {"model": "App.cryptographyarticle", "pk": 25, "fields": {"title": "Flow Exporter Impact on Intelligent Intrusion Detection Systems", "date": "2024-12-18", "authors": "Daniela Pinto, João Vitorino, Eva Maia, Ivone Amorim, Isabel Praça", "content": "High-quality datasets are critical for training machine learning models, as\ninconsistencies in feature generation can hinder the accuracy and reliability\nof threat detection. For this reason, ensuring the quality of the data in\nnetwork intrusion detection datasets is important. A key component of this is\nusing reliable tools to generate the flows and features present in the\ndatasets. This paper investigates the impact of flow exporters on the\nperformance and reliability of machine learning models for intrusion detection.\nUsing HERA, a tool designed to export flows and extract features, the raw\nnetwork packets of two widely used datasets, UNSW-NB15 and CIC-IDS2017, were\nprocessed from PCAP files to generate new versions of these datasets. These\nwere compared to the original ones in terms of their influence on the\nperformance of several models, including Random Forest, XGBoost, LightGBM, and\nExplainable Boosting Machine. The results obtained were significant. Models\ntrained on the HERA version of the datasets consistently outperformed those\ntrained on the original dataset, showing improvements in accuracy and\nindicating a better generalisation. This highlighted the importance of flow\ngeneration in the model's ability to differentiate between benign and malicious\ntraffic.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.14021v1"}}, {"model": "App.cryptographyarticle", "pk": 26, "fields": {"title": "Resilience of Networks to Spreading Computer Viruses: Optimal Anti-Virus Deployment (Extended Version)", "date": "2024-12-18", "authors": "Jhonatan Tavori, Hanoch Levy", "content": "Deployment of anti-virus software is a common strategy for preventing and\ncontrolling the propagation of computer viruses and worms over a computer\nnetwork. As the deployment of such programs is often limited due to monetary or\noperational costs, devising optimal strategies for their allocation and\ndeployment can be of high value to the operation, performance, and resilience\nof the target networks.\n  We study the effects of anti-virus deployment (i.e., \"vaccination\")\nstrategies on the ability of a network to block the spread of a virus. Such\nability is obtained when the network reaches \"herd immunity\", achieved when a\nlarge fraction of the network entities is immune to the infection, which\nprovides protection even for entities which are not immune. We use a model that\nexplicitly accounts for the inherent heterogeneity of network nodes activity\nand derive optimal strategies for anti-virus deployment.\n  Numerical evaluations demonstrate that the system performance is very\nsensitive to the chosen strategy, and thus strategies which disregard the\nheterogeneous spread nature may perform significantly worse relatively to those\nderived in this work.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13911v1"}}, {"model": "App.cryptographyarticle", "pk": 27, "fields": {"title": "T-Edge: Trusted Heterogeneous Edge Computing", "date": "2024-12-18", "authors": "Jiamin Shen, Yao Chen, Weng-Fai Wong, Ee-Chien Chang", "content": "Heterogeneous computing, which incorporates GPUs, NPUs, and FPGAs, is\nincreasingly utilized to improve the efficiency of computer systems. However,\nthis shift has given rise to significant security and privacy concerns,\nespecially when the execution platform is remote. One way to tackle these\nchallenges is to establish a trusted and isolated environment for remote\nprogram execution, while maintaining minimal overhead and flexibility. While\nCPU-based trusted execution has been extensively explored and found commercial\nsuccess, extension to heterogeneous computing systems remains a challenge. This\npaper proposes a practical trusted execution environment design for ARM/FPGA\nSystem-on-Chip platforms, leveraging TrustZone's unique characteristics. The\ndesign features a dedicated security controller within the ARM TrustZone,\noverseeing FPGA reconfiguration and managing communication between CPU cores\nand FPGA fabrics. This design involves a provisioning service that enables\napplication users to establish trust in the FPGA fabric within cloud-based\ncomputing resources provided by the platform owner, running applications\ndeveloped by third-party developers and hardware manufactured by the device\nmanufacturer. To ensure the security of our proposed system, we employ an\nautomated protocol verifier, ProVerif, to validate its compliance with\nessential security requirements. Furthermore, we demonstrate the practicality\nof our system model by implementing a prototype application on the Xilinx MPSoC\ndevelopment board.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13905v1"}}, {"model": "App.cryptographyarticle", "pk": 28, "fields": {"title": "A Review of the Duality of Adversarial Learning in Network Intrusion: Attacks and Countermeasures", "date": "2024-12-18", "authors": "Shalini Saini, Anitha Chennamaneni, Babatunde Sawyerr", "content": "Deep learning solutions are instrumental in cybersecurity, harnessing their\nability to analyze vast datasets, identify complex patterns, and detect\nanomalies. However, malevolent actors can exploit these capabilities to\norchestrate sophisticated attacks, posing significant challenges to defenders\nand traditional security measures. Adversarial attacks, particularly those\ntargeting vulnerabilities in deep learning models, present a nuanced and\nsubstantial threat to cybersecurity. Our study delves into adversarial learning\nthreats such as Data Poisoning, Test Time Evasion, and Reverse Engineering,\nspecifically impacting Network Intrusion Detection Systems. Our research\nexplores the intricacies and countermeasures of attacks to deepen understanding\nof network security challenges amidst adversarial threats. In our study, we\npresent insights into the dynamic realm of adversarial learning and its\nimplications for network intrusion. The intersection of adversarial attacks and\ndefenses within network traffic data, coupled with advances in machine learning\nand deep learning techniques, represents a relatively underexplored domain. Our\nresearch lays the groundwork for strengthening defense mechanisms to address\nthe potential breaches in network security and privacy posed by adversarial\nattacks. Through our in-depth analysis, we identify domain-specific research\ngaps, such as the scarcity of real-life attack data and the evaluation of\nAI-based solutions for network traffic. Our focus on these challenges aims to\nstimulate future research efforts toward the development of resilient network\ndefense strategies.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13880v1"}}, {"model": "App.cryptographyarticle", "pk": 29, "fields": {"title": "Crabs: Consuming Resrouce via Auto-generation for LLM-DoS Attack under Black-box Settings", "date": "2024-12-18", "authors": "Yuanhe Zhang, Zhenhong Zhou, Wei Zhang, Xinyue Wang, Xiaojun Jia, Yang Liu, Sen Su", "content": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks. LLMs continue to be vulnerable to external threats, particularly\nDenial-of-Service (DoS) attacks. Specifically, LLM-DoS attacks aim to exhaust\ncomputational resources and block services. However, prior works tend to focus\non performing white-box attacks, overlooking black-box settings. In this work,\nwe propose an automated algorithm designed for black-box LLMs, called\nAuto-Generation for LLM-DoS Attack (AutoDoS). AutoDoS introduces DoS Attack\nTree and optimizes the prompt node coverage to enhance effectiveness under\nblack-box conditions. Our method can bypass existing defense with enhanced\nstealthiness via semantic improvement of prompt nodes. Furthermore, we reveal\nthat implanting Length Trojan in Basic DoS Prompt aids in achieving higher\nattack efficacy. Experimental results show that AutoDoS amplifies service\nresponse latency by over 250 $\\times \\uparrow$, leading to severe resource\nconsumption in terms of GPU utilization and memory usage. Our code is available\nat \\url{https://github.com/shuita2333/AutoDoS}.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13879v1"}}, {"model": "App.cryptographyarticle", "pk": 30, "fields": {"title": "Towards an identity management solution on Arweave", "date": "2024-12-19", "authors": "Andreea Elena Dragnoiu, Ruxandra F. Olimid", "content": "Traditional identity management systems, often centralized, face challenges\naround privacy, data security, and user control, leaving users vulnerable to\ndata breaches and misuse. This paper explores the potential of using the\nArweave network to develop an identity management solution. By harnessing\nArweave's permanent storage, our solution offers the users a Self-Sovereign\nIdentity (SSI) framework, that uses Decentralized Identifiers (DIDs) and\nVerifiable Credentials (VCs) to allow individuals and other entities to create,\nown, and manage their digital identities. Further, the solution integrates\nprivacy-preserving technologies, including zero-knowledge proofs and the BBS(+)\nsignature scheme, enabling selective disclosure. This approach ultimately\nenhances user privacy and supports compliance with European Union legislation\nand regulatory standards like the General Data Protection Regulation (GDPR) by\ndesign.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13865v2"}}, {"model": "App.cryptographyarticle", "pk": 31, "fields": {"title": "Fed-AugMix: Balancing Privacy and Utility via Data Augmentation", "date": "2024-12-18", "authors": "Haoyang Li, Wei Chen, Xiaojin Zhang", "content": "Gradient leakage attacks pose a significant threat to the privacy guarantees\nof federated learning. While distortion-based protection mechanisms are\ncommonly employed to mitigate this issue, they often lead to notable\nperformance degradation. Existing methods struggle to preserve model\nperformance while ensuring privacy. To address this challenge, we propose a\nnovel data augmentation-based framework designed to achieve a favorable\nprivacy-utility trade-off, with the potential to enhance model performance in\ncertain cases. Our framework incorporates the AugMix algorithm at the client\nlevel, enabling data augmentation with controllable severity. By integrating\nthe Jensen-Shannon divergence into the loss function, we embed the distortion\nintroduced by AugMix into the model gradients, effectively safeguarding privacy\nagainst deep leakage attacks. Moreover, the JS divergence promotes model\nconsistency across different augmentations of the same image, enhancing both\nrobustness and performance. Extensive experiments on benchmark datasets\ndemonstrate the effectiveness and stability of our method in protecting\nprivacy. Furthermore, our approach maintains, and in some cases improves, model\nperformance, showcasing its ability to achieve a robust privacy-utility\ntrade-off.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13818v1"}}, {"model": "App.cryptographyarticle", "pk": 32, "fields": {"title": "Differentially Private Substring and Document Counting", "date": "2024-12-18", "authors": "Giulia Bernardini, Philip Bille, Inge Li Gørtz, Teresa Anna Steiner", "content": "Differential privacy is the gold standard for privacy in data analysis. In\nmany data analysis applications, the data is a database of documents. For\ndatabases consisting of many documents, one of the most fundamental problems is\nthat of pattern matching and computing (i) how often a pattern appears as a\nsubstring in the database (substring counting) and (ii) how many documents in\nthe collection contain the pattern as a substring (document counting). In this\npaper, we initiate the theoretical study of substring and document counting\nunder differential privacy.\n  We give an $\\epsilon$-differentially private data structure solving this\nproblem for all patterns simultaneously with a maximum additive error of\n$O(\\ell \\cdot\\mathrm{polylog}(n\\ell|\\Sigma|))$, where $\\ell$ is the maximum\nlength of a document in the database, $n$ is the number of documents, and\n$|\\Sigma|$ is the size of the alphabet. We show that this is optimal up to a\n$O(\\mathrm{polylog}(n\\ell))$ factor. Further, we show that for\n$(\\epsilon,\\delta)$-differential privacy, the bound for document counting can\nbe improved to $O(\\sqrt{\\ell} \\cdot\\mathrm{polylog}(n\\ell|\\Sigma|))$.\nAdditionally, our data structures are efficient. In particular, our data\nstructures use $O(n\\ell^2)$ space, $O(n^2\\ell^4)$ preprocessing time, and\n$O(|P|)$ query time where $P$ is the query pattern. Along the way, we develop a\nnew technique for differentially privately computing a general class of\ncounting functions on trees of independent interest.\n  Our data structures immediately lead to improved algorithms for related\nproblems, such as privately mining frequent substrings and $q$-grams. For\n$q$-grams, we further improve the preprocessing time of the data structure.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13813v1"}}, {"model": "App.cryptographyarticle", "pk": 33, "fields": {"title": "Data sharing in the metaverse with key abuse resistance based on decentralized CP-ABE", "date": "2024-12-18", "authors": "Liang Zhang, Zhanrong Ou, Changhui Hu, Haibin Kan, Jiheng Zhang", "content": "Data sharing is ubiquitous in the metaverse, which adopts blockchain as its\nfoundation. Blockchain is employed because it enables data transparency,\nachieves tamper resistance, and supports smart contracts. However, securely\nsharing data based on blockchain necessitates further consideration.\nCiphertext-policy attribute-based encryption (CP-ABE) is a promising primitive\nto provide confidentiality and fine-grained access control. Nonetheless,\nauthority accountability and key abuse are critical issues that practical\napplications must address. Few studies have considered CP-ABE key\nconfidentiality and authority accountability simultaneously. To our knowledge,\nwe are the first to fill this gap by integrating non-interactive zero-knowledge\n(NIZK) proofs into CP-ABE keys and outsourcing the verification process to a\nsmart contract. To meet the decentralization requirement, we incorporate a\ndecentralized CP-ABE scheme into the proposed data sharing system.\nAdditionally, we provide an implementation based on smart contract to determine\nwhether an access control policy is satisfied by a set of CP-ABE keys. We also\nintroduce an open incentive mechanism to encourage honest participation in data\nsharing. Hence, the key abuse issue is resolved through the NIZK proof and the\nincentive mechanism. We provide a theoretical analysis and conduct\ncomprehensive experiments to demonstrate the feasibility and efficiency of the\ndata sharing system. Based on the proposed accountable approach, we further\nillustrate an application in GameFi, where players can play to earn or\ncontribute to an accountable DAO, fostering a thriving metaverse ecosystem.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13770v1"}}, {"model": "App.cryptographyarticle", "pk": 34, "fields": {"title": "Clio: Privacy-Preserving Insights into Real-World AI Use", "date": "2024-12-18", "authors": "Alex Tamkin, Miles McCain, Kunal Handa, Esin Durmus, Liane Lovitt, Ankur Rathi, Saffron Huang, Alfred Mountfield, Jerry Hong, Stuart Ritchie, Michael Stern, Brian Clarke, Landon Goldberg, Theodore R. Sumers, Jared Mueller, William McEachen, Wes Mitchell, Shan Carter, Jack Clark, Jared Kaplan, Deep Ganguli", "content": "How are AI assistants being used in the real world? While model providers in\ntheory have a window into this impact via their users' data, both privacy\nconcerns and practical challenges have made analyzing this data difficult. To\naddress these issues, we present Clio (Claude insights and observations), a\nprivacy-preserving platform that uses AI assistants themselves to analyze and\nsurface aggregated usage patterns across millions of conversations, without the\nneed for human reviewers to read raw conversations. We validate this can be\ndone with a high degree of accuracy and privacy by conducting extensive\nevaluations. We demonstrate Clio's usefulness in two broad ways. First, we\nshare insights about how models are being used in the real world from one\nmillion Claude.ai Free and Pro conversations, ranging from providing advice on\nhairstyles to providing guidance on Git operations and concepts. We also\nidentify the most common high-level use cases on Claude.ai (coding, writing,\nand research tasks) as well as patterns that differ across languages (e.g.,\nconversations in Japanese discuss elder care and aging populations at\nhigher-than-typical rates). Second, we use Clio to make our systems safer by\nidentifying coordinated attempts to abuse our systems, monitoring for unknown\nunknowns during critical periods like launches of new capabilities or major\nworld events, and improving our existing monitoring systems. We also discuss\nthe limitations of our approach, as well as risks and ethical concerns. By\nenabling analysis of real-world AI usage, Clio provides a scalable platform for\nempirically grounded AI safety and governance.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13678v1"}}, {"model": "App.cryptographyarticle", "pk": 35, "fields": {"title": "Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration", "date": "2024-12-18", "authors": "Xuhan Zuo, Minghao Wang, Tianqing Zhu, Shui Yu, Wanlei Zhou", "content": "Large language models (LLMs) have transformed the way computers understand\nand process human language, but using them effectively across different\norganizations remains still difficult. When organizations work together to\nimprove LLMs, they face several main challenges. First, organizations hesitate\nto share their valuable data with others. Second, competition between\norganizations creates trust problems during collaboration. Third, new privacy\nlaws require organizations to be able to delete specific data when requested,\nwhich is especially difficult when multiple organizations are learning from\nshared data. Traditional federated learning approaches do not address these\ninterconnected challenges, particularly in scenarios where participants cannot\nfully trust each other or the central aggregator. To overcome these\nlimitations, we propose a hybrid blockchain-based federated learning framework\nthat uniquely combines public and private blockchain architectures with\nmulti-agent reinforcement learning. Our framework enables transparent sharing\nof model update through the public blockchain while protecting sensitive\ncomputations in private chains. Each organization operates as an intelligent\nagent, using Q-learning to optimize its participation strategy and resource\nallocation, thus aligning individual incentives with collective goals. Notably,\nwe introduce an efficient unlearning mechanism based on Low-Rank Adaptation\n(LoRA) that enables selective removal of specific data contributions without\ncompromising the model's overall performance. Through extensive experimentation\non real-world datasets, we demonstrate that our framework effectively balances\nprivacy protection, trust establishment, and regulatory compliance while\nmaintaining high model performance.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13551v1"}}, {"model": "App.cryptographyarticle", "pk": 36, "fields": {"title": "Privacy-Preserving Cyberattack Detection in Blockchain-Based IoT Systems Using AI and Homomorphic Encryption", "date": "2024-12-18", "authors": "Bui Duc Manh, Chi-Hieu Nguyen, Dinh Thai Hoang, Diep N. Nguyen, Ming Zeng, Quoc-Viet Pham", "content": "This work proposes a novel privacy-preserving cyberattack detection framework\nfor blockchain-based Internet-of-Things (IoT) systems. In our approach,\nartificial intelligence (AI)-driven detection modules are strategically\ndeployed at blockchain nodes to identify real-time attacks, ensuring high\naccuracy and minimal delay. To achieve this efficiency, the model training is\nconducted by a cloud service provider (CSP). Accordingly, blockchain nodes send\ntheir data to the CSP for training, but to safeguard privacy, the data is\nencrypted using homomorphic encryption (HE) before transmission. This\nencryption method allows the CSP to perform computations directly on encrypted\ndata without the need for decryption, preserving data privacy throughout the\nlearning process. To handle the substantial volume of encrypted data, we\nintroduce an innovative packing algorithm in a Single-Instruction-Multiple-Data\n(SIMD) manner, enabling efficient training on HE-encrypted data. Building on\nthis, we develop a novel deep neural network training algorithm optimized for\nencrypted data. We further propose a privacy-preserving distributed learning\napproach based on the FedAvg algorithm, which parallelizes the training across\nmultiple workers, significantly improving computation time. Upon completion,\nthe CSP distributes the trained model to the blockchain nodes, enabling them to\nperform real-time, privacy-preserved detection. Our simulation results\ndemonstrate that our proposed method can not only mitigate the training time\nbut also achieve detection accuracy that is approximately identical to the\napproach without encryption, with a gap of around 0.01%. Additionally, our real\nimplementations on various blockchain consensus algorithms and hardware\nconfigurations show that our proposed framework can also be effectively adapted\nto real-world systems.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13522v1"}}, {"model": "App.cryptographyarticle", "pk": 37, "fields": {"title": "4.5 Million (Suspected) Fake Stars in GitHub: A Growing Spiral of Popularity Contests, Scams, and Malware", "date": "2024-12-18", "authors": "Hao He, Haoqin Yang, Philipp Burckhardt, Alexandros Kapravelos, Bogdan Vasilescu, Christian Kästner", "content": "GitHub, the de-facto platform for open-source software development, provides\na set of social-media-like features to signal high-quality repositories. Among\nthem, the star count is the most widely used popularity signal, but it is also\nat risk of being artificially inflated (i.e., faked), decreasing its value as a\ndecision-making signal and posing a security risk to all GitHub users. In this\npaper, we present a systematic, global, and longitudinal measurement study of\nfake stars in GitHub. To this end, we build StarScout, a scalable tool able to\ndetect anomalous starring behaviors (i.e., low activity and lockstep) across\nthe entire GitHub metadata. Analyzing the data collected using StarScout, we\nfind that: (1) fake-star-related activities have rapidly surged since 2024; (2)\nthe user profile characteristics of fake stargazers are not distinct from\naverage GitHub users, but many of them have highly abnormal activity patterns;\n(3) the majority of fake stars are used to promote short-lived malware\nrepositories masquerading as pirating software, game cheats, or cryptocurrency\nbots; (4) some repositories may have acquired fake stars for growth hacking,\nbut fake stars only have a promotion effect in the short term (i.e., less than\ntwo months) and become a burden in the long term. Our study has implications\nfor platform moderators, open-source practitioners, and supply chain security\nresearchers.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13459v1"}}, {"model": "App.cryptographyarticle", "pk": 38, "fields": {"title": "Toward an Insider Threat Education Platform: A Theoretical Literature Review", "date": "2024-12-18", "authors": "Haywood Gelman, John D. Hastings, David Kenley, Eleanor Loiacono", "content": "Insider threats (InTs) within organizations are small in number but have a\ndisproportionate ability to damage systems, information, and infrastructure.\nExisting InT research studies the problem from psychological, technical, and\neducational perspectives. Proposed theories include research on psychological\nindicators, machine learning, user behavioral log analysis, and educational\nmethods to teach employees recognition and mitigation techniques. Because InTs\nare a human problem, training methods that address InT detection from a\nbehavioral perspective are critical. While numerous technological and\npsychological theories exist on detection, prevention, and mitigation, few\ntraining methods prioritize psychological indicators. This literature review\nstudied peer-reviewed, InT research organized by subtopic and extracted\ncritical theories from psychological, technical, and educational disciplines.\nIn doing so, this is the first study to comprehensively organize research\nacross all three approaches in a manner which properly informs the development\nof an InT education platform.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13446v1"}}, {"model": "App.cryptographyarticle", "pk": 39, "fields": {"title": "Safeguarding Virtual Healthcare: A Novel Attacker-Centric Model for Data Security and Privacy", "date": "2024-12-18", "authors": "Suvineetha Herath, Haywood Gelman, John Hastings, Yong Wang", "content": "The rapid growth of remote healthcare delivery has introduced significant\nsecurity and privacy risks to protected health information (PHI). Analysis of a\ncomprehensive healthcare security breach dataset covering 2009-2023 reveals\ntheir significant prevalence and impact. This study investigates the root\ncauses of such security incidents and introduces the Attacker-Centric Approach\n(ACA), a novel threat model tailored to protect PHI. ACA addresses limitations\nin existing threat models and regulatory frameworks by adopting a holistic\nattacker-focused perspective, examining threats from the viewpoint of cyber\nadversaries, their motivations, tactics, and potential attack vectors.\nLeveraging established risk management frameworks, ACA provides a multi-layered\napproach to threat identification, risk assessment, and proactive mitigation\nstrategies. A comprehensive threat library classifies physical, third-party,\nexternal, and internal threats. ACA's iterative nature and feedback mechanisms\nenable continuous adaptation to emerging threats, ensuring sustained\neffectiveness. ACA allows healthcare providers to proactively identify and\nmitigate vulnerabilities, fostering trust and supporting the secure adoption of\nvirtual care technologies.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13440v1"}}, {"model": "App.cryptographyarticle", "pk": 40, "fields": {"title": "Magnifier: Detecting Network Access via Lightweight Traffic-based Fingerprints", "date": "2024-12-18", "authors": "Wenhao Li, Qiang Wang, Huaifeng Bao, Xiao-Yu Zhang, Lingyun Ying, Zhaoxuan Li", "content": "Network access detection plays a crucial role in global network management,\nenabling efficient network monitoring and topology measurement by identifying\nunauthorized network access and gathering detailed information about mobile\ndevices. Existing methods for endpoint-based detection primarily rely on\ndeploying monitoring software to recognize network connections. However, the\nchallenges associated with developing and maintaining such systems have limited\ntheir universality and coverage in practical deployments, especially given the\ncost implications of covering a wide array of devices with heterogeneous\noperating systems. To tackle the issues, we propose Magnifier for mobile device\nnetwork access detection that, for the first time, passively infers access\npatterns from backbone traffic at the gateway level. Magnifier's foundation is\nthe creation of device-specific access patterns using the innovative Domain\nName Forest (dnForest) fingerprints. We then employ a two-stage distillation\nalgorithm to fine-tune the weights of individual Domain Name Trees (dnTree)\nwithin each dnForest, emphasizing the unique device fingerprints. With these\nmeticulously crafted fingerprints, Magnifier efficiently infers network access\nfrom backbone traffic using a lightweight fingerprint matching algorithm. Our\nexperimental results, conducted in real-world scenarios, demonstrate that\nMagnifier exhibits exceptional universality and coverage in both initial and\nrepetitive network access detection in real-time. To facilitate further\nresearch, we have thoughtfully curated the NetCess2023 dataset, comprising\nnetwork access data from 26 different models across 7 brands, covering the\nmajority of mainstream mobile devices. We have also made both the Magnifier\nprototype and the NetCess2023 dataset publicly\navailable\\footnote{https://github.com/SecTeamPolaris/Magnifier}.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13428v1"}}, {"model": "App.cryptographyarticle", "pk": 41, "fields": {"title": "Safeguarding System Prompts for LLMs", "date": "2024-12-18", "authors": "Zhifeng Jiang, Zhihua Jin, Guoliang He", "content": "Large language models (LLMs) are increasingly utilized in applications where\nsystem prompts, which guide model outputs, play a crucial role. These prompts\noften contain business logic and sensitive information, making their protection\nessential. However, adversarial and even regular user queries can exploit LLM\nvulnerabilities to expose these hidden prompts. To address this issue, we\npresent PromptKeeper, a novel defense mechanism for system prompt privacy. By\nreliably detecting worst-case leakage and regenerating outputs without the\nsystem prompt when necessary, PromptKeeper ensures robust protection against\nprompt extraction attacks via either adversarial or regular queries, while\npreserving conversational capability and runtime efficiency during benign user\ninteractions.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13426v1"}}, {"model": "App.cryptographyarticle", "pk": 42, "fields": {"title": "Targeted View-Invariant Adversarial Perturbations for 3D Object Recognition", "date": "2024-12-17", "authors": "Christian Green, Mehmet Ergezer, Abdurrahman Zeybey", "content": "Adversarial attacks pose significant challenges in 3D object recognition,\nespecially in scenarios involving multi-view analysis where objects can be\nobserved from varying angles. This paper introduces View-Invariant Adversarial\nPerturbations (VIAP), a novel method for crafting robust adversarial examples\nthat remain effective across multiple viewpoints. Unlike traditional methods,\nVIAP enables targeted attacks capable of manipulating recognition systems to\nclassify objects as specific, pre-determined labels, all while using a single\nuniversal perturbation. Leveraging a dataset of 1,210 images across 121 diverse\nrendered 3D objects, we demonstrate the effectiveness of VIAP in both targeted\nand untargeted settings. Our untargeted perturbations successfully generate a\nsingular adversarial noise robust to 3D transformations, while targeted attacks\nachieve exceptional results, with top-1 accuracies exceeding 95% across various\nepsilon values. These findings highlight VIAPs potential for real-world\napplications, such as testing the robustness of 3D recognition systems. The\nproposed method sets a new benchmark for view-invariant adversarial robustness,\nadvancing the field of adversarial machine learning for 3D object recognition.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13376v1"}}, {"model": "App.cryptographyarticle", "pk": 43, "fields": {"title": "GraphFuzz: Accelerating Hardware Testing with Graph Models", "date": "2024-12-17", "authors": "Raghul Saravanan, Sreenitha Kasarapu, Sai Manoj Pudukotai Dinakarrao", "content": "The increasing complexity of modern processor and IP designs presents\nsignificant challenges in identifying and mitigating hardware flaws early in\nthe IC design cycle. Traditional hardware fuzzing techniques, inspired by\nsoftware testing, have shown promise but face scalability issues, especially at\nthe gate-level netlist where bugs introduced during synthesis are often missed\nby RTL-level verification due to longer simulation times.\n  To address this, we introduce GraphFuzz, a graph-based hardware fuzzer\ndesigned for gate-level netlist verification. In this approach, hardware\ndesigns are modeled as graph nodes, with gate behaviors encoded as features. By\nleveraging graph learning algorithms, GraphFuzz efficiently detects hardware\nvulnerabilities by analyzing node patterns. Our evaluation across benchmark\ncircuits and open-source processors demonstrates an average prediction accuracy\nof 80% and bug detection accuracy of 70%, highlighting the potential of\ngraph-based methods for enhancing hardware verification.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13374v1"}}, {"model": "App.cryptographyarticle", "pk": 44, "fields": {"title": "Key Exchange in the Quantum Era: Evaluating a Hybrid System of Public-Key Cryptography and Physical-Layer Security", "date": "2024-12-17", "authors": "Paul Staat, Meik Dörpinghaus, Azadeh Sheikholeslami, Christof Paar, Gerhard Fettweis, Dennis Goeckel", "content": "Today's information society relies on cryptography to achieve security goals\nsuch as confidentiality, integrity, authentication, and non-repudiation for\ndigital communications. Here, public-key cryptosystems play a pivotal role to\nshare encryption keys and create digital signatures. However, quantum computers\nthreaten the security of traditional public-key cryptosystems as they can tame\ncomputational problems underlying the schemes, i.e., discrete logarithm and\ninteger factorization. The prospective arrival of capable-enough quantum\ncomputers already threatens today's secret communication in terms of their\nlong-term secrecy when stored to be later decrypted. Therefore, researchers\nstrive to develop and deploy alternative schemes.\n  In this work, evaluate a key exchange protocol based on combining public-key\nschemes with physical-layer security, anticipating the prospect of quantum\nattacks. If powerful quantum attackers cannot immediately obtain private keys,\nlegitimate parties have a window of short-term secrecy to perform a\nphysical-layer jamming key exchange (JKE) to establish a long-term shared\nsecret. Thereby, the protocol constraints the computation time available to the\nattacker to break the employed public-key cryptography. In this paper, we\noutline the protocol, discuss its security, and point out challenges to be\nresolved.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13352v1"}}, {"model": "App.cryptographyarticle", "pk": 45, "fields": {"title": "Concept-ROT: Poisoning Concepts in Large Language Models with Model Editing", "date": "2024-12-17", "authors": "Keltin Grimes, Marco Christiani, David Shriver, Marissa Connor", "content": "Model editing methods modify specific behaviors of Large Language Models by\naltering a small, targeted set of network weights and require very little data\nand compute. These methods can be used for malicious applications such as\ninserting misinformation or simple trojans that result in adversary-specified\nbehaviors when a trigger word is present. While previous editing methods have\nfocused on relatively constrained scenarios that link individual words to fixed\noutputs, we show that editing techniques can integrate more complex behaviors\nwith similar effectiveness. We develop Concept-ROT, a model editing-based\nmethod that efficiently inserts trojans which not only exhibit complex output\nbehaviors, but also trigger on high-level concepts -- presenting an entirely\nnew class of trojan attacks. Specifically, we insert trojans into frontier\nsafety-tuned LLMs which trigger only in the presence of concepts such as\n'computer science' or 'ancient civilizations.' When triggered, the trojans\njailbreak the model, causing it to answer harmful questions that it would\notherwise refuse. Our results further motivate concerns over the practicality\nand potential ramifications of trojan attacks on Machine Learning models.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13341v1"}}, {"model": "App.cryptographyarticle", "pk": 46, "fields": {"title": "BadSAD: Clean-Label Backdoor Attacks against Deep Semi-Supervised Anomaly Detection", "date": "2024-12-17", "authors": "He Cheng, Depeng Xu, Shuhan Yuan", "content": "Image anomaly detection (IAD) is essential in applications such as industrial\ninspection, medical imaging, and security. Despite the progress achieved with\ndeep learning models like Deep Semi-Supervised Anomaly Detection (DeepSAD),\nthese models remain susceptible to backdoor attacks, presenting significant\nsecurity challenges. In this paper, we introduce BadSAD, a novel backdoor\nattack framework specifically designed to target DeepSAD models. Our approach\ninvolves two key phases: trigger injection, where subtle triggers are embedded\ninto normal images, and latent space manipulation, which positions and clusters\nthe poisoned images near normal images to make the triggers appear benign.\nExtensive experiments on benchmark datasets validate the effectiveness of our\nattack strategy, highlighting the severe risks that backdoor attacks pose to\ndeep learning-based anomaly detection systems.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13324v1"}}, {"model": "App.cryptographyarticle", "pk": 47, "fields": {"title": "TETRIS: Composing FHE Techniques for Private Functional Exploration Over Large Datasets", "date": "2024-12-17", "authors": "Malika Izabachène, Jean-Philippe Bossuat", "content": "To derive valuable insights from statistics, machine learning applications\nfrequently analyze substantial amounts of data. In this work, we address the\nproblem of designing efficient secure techniques to probe large datasets which\nallow a scientist to conduct large-scale medical studies over specific\nattributes of patients' records, while maintaining the privacy of his model. We\nintroduce a set of composable homomorphic operations and show how to combine\nprivate functions evaluation with private thresholds via approximate fully\nhomomorphic encryption. This allows us to design a new system named TETRIS,\nwhich solves the real-world use case of private functional exploration of large\ndatabases, where the statistical criteria remain private to the server owning\nthe patients' records. Our experiments show that TETRIS achieves practical\nperformance over a large dataset of patients even for the evaluation of\nelaborate statements composed of linear and nonlinear functions. It is possible\nto extract private insights from a database of hundreds of thousands of patient\nrecords within only a few minutes on a single thread, with an amortized time\nper database entry smaller than 2ms.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13269v1"}}, {"model": "App.cryptographyarticle", "pk": 48, "fields": {"title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents", "date": "2024-12-18", "authors": "Sheng Yin, Xianghe Pang, Yuanzhuo Ding, Menglan Chen, Yutong Bi, Yichen Xiong, Wenhao Huang, Zhen Xiang, Jing Shao, Siheng Chen", "content": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to execute complicated instructions in natural language,\npaving a way for the potential deployment of embodied robots. However, a\nforeseeable issue is that those embodied agents can also flawlessly execute\nsome hazardous tasks, potentially causing damages in real world. To study this\nissue, we present SafeAgentBench -- a new benchmark for safety-aware task\nplanning of embodied LLM agents. SafeAgentBench includes: (1) a new dataset\nwith 750 tasks, covering 10 potential hazards and 3 task types; (2)\nSafeAgentEnv, a universal embodied environment with a low-level controller,\nsupporting multi-agent execution with 17 high-level actions for 8\nstate-of-the-art baselines; and (3) reliable evaluation methods from both\nexecution and semantic perspectives. Experimental results show that the\nbest-performing baseline gets 69% success rate for safe tasks, but only 5%\nrejection rate for hazardous tasks, indicating significant safety risks. More\ndetails and codes are available at\nhttps://github.com/shengyin1224/SafeAgentBench.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13178v2"}}, {"model": "App.cryptographyarticle", "pk": 49, "fields": {"title": "Practicable Black-box Evasion Attacks on Link Prediction in Dynamic Graphs -- A Graph Sequential Embedding Method", "date": "2024-12-17", "authors": "Jiate Li, Meng Pang, Binghui Wang", "content": "Link prediction in dynamic graphs (LPDG) has been widely applied to\nreal-world applications such as website recommendation, traffic flow\nprediction, organizational studies, etc. These models are usually kept local\nand secure, with only the interactive interface restrictively available to the\npublic. Thus, the problem of the black-box evasion attack on the LPDG model,\nwhere model interactions and data perturbations are restricted, seems to be\nessential and meaningful in practice. In this paper, we propose the first\npracticable black-box evasion attack method that achieves effective attacks\nagainst the target LPDG model, within a limited amount of interactions and\nperturbations. To perform effective attacks under limited perturbations, we\ndevelop a graph sequential embedding model to find the desired state embedding\nof the dynamic graph sequences, under a deep reinforcement learning framework.\nTo overcome the scarcity of interactions, we design a multi-environment\ntraining pipeline and train our agent for multiple instances, by sharing an\naggregate interaction buffer. Finally, we evaluate our attack against three\nadvanced LPDG models on three real-world graph datasets of different scales and\ncompare its performance with related methods under the interaction and\nperturbation constraints. Experimental results show that our attack is both\neffective and practicable.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13134v1"}}, {"model": "App.cryptographyarticle", "pk": 50, "fields": {"title": "Enhancing Internet of Things Security throughSelf-Supervised Graph Neural Networks", "date": "2024-12-17", "authors": "Safa Ben Atitallah, Maha Driss, Wadii Boulila, Anis Koubaa", "content": "With the rapid rise of the Internet of Things (IoT), ensuring the security of\nIoT devices has become essential. One of the primary challenges in this field\nis that new types of attacks often have significantly fewer samples than more\ncommon attacks, leading to unbalanced datasets. Existing research on detecting\nintrusions in these unbalanced labeled datasets primarily employs Convolutional\nNeural Networks (CNNs) or conventional Machine Learning (ML) models, which\nresult in incomplete detection, especially for new attacks. To handle these\nchallenges, we suggest a new approach to IoT intrusion detection using\nSelf-Supervised Learning (SSL) with a Markov Graph Convolutional Network\n(MarkovGCN). Graph learning excels at modeling complex relationships within\ndata, while SSL mitigates the issue of limited labeled data for emerging\nattacks. Our approach leverages the inherent structure of IoT networks to\npre-train a GCN, which is then fine-tuned for the intrusion detection task. The\nintegration of Markov chains in GCN uncovers network structures and enriches\nnode and edge features with contextual information. Experimental results\ndemonstrate that our approach significantly improves detection accuracy and\nrobustness compared to conventional supervised learning methods. Using the\nEdgeIIoT-set dataset, we attained an accuracy of 98.68\\%, a precision of\n98.18%, a recall of 98.35%, and an F1-Score of 98.40%.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13240v1"}}, {"model": "App.cryptographyarticle", "pk": 51, "fields": {"title": "Accuracy Limits as a Barrier to Biometric System Security", "date": "2024-12-19", "authors": "Axel Durbet, Paul-Marie Grollemund, Pascal Lafourcade, Kevin Thiry-Atighehchi", "content": "Biometric systems are widely used for identity verification and\nidentification, including authentication (i.e., one-to-one matching to verify a\nclaimed identity) and identification (i.e., one-to-many matching to find a\nsubject in a database). The matching process relies on measuring similarities\nor dissimilarities between a fresh biometric template and enrolled templates.\nThe False Match Rate FMR is a key metric for assessing the accuracy and\nreliability of such systems. This paper analyzes biometric systems based on\ntheir FMR, with two main contributions. First, we explore untargeted attacks,\nwhere an adversary aims to impersonate any user within a database. We determine\nthe number of trials required for an attacker to successfully impersonate a\nuser and derive the critical population size (i.e., the maximum number of users\nin the database) required to maintain a given level of security. Furthermore,\nwe compute the critical FMR value needed to ensure resistance against\nuntargeted attacks as the database size increases. Second, we revisit the\nbiometric birthday problem to evaluate the approximate and exact probabilities\nthat two users in a database collide (i.e., can impersonate each other). Based\non this analysis, we derive both the approximate critical population size and\nthe critical FMR value needed to bound the likelihood of such collisions\noccurring with a given probability. These thresholds offer insights for\ndesigning systems that mitigate the risk of impersonation and collisions,\nparticularly in large-scale biometric databases. Our findings indicate that\ncurrent biometric systems fail to deliver sufficient accuracy to achieve an\nadequate security level against untargeted attacks, even in small-scale\ndatabases. Moreover, state-of-the-art systems face significant challenges in\naddressing the biometric birthday problem, especially as database sizes grow.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13099v2"}}, {"model": "App.cryptographyarticle", "pk": 52, "fields": {"title": "TIMESAFE: Timing Interruption Monitoring and Security Assessment for Fronthaul Environments", "date": "2024-12-17", "authors": "Joshua Groen, Simone Di Valerio, Imtiaz Karim, Davide Villa, Yiewi Zhang, Leonardo Bonati, Michele Polese, Salvatore D'Oro, Tommaso Melodia, Elisa Bertino, Francesca Cuomo, Kaushik Chowdhury", "content": "5G and beyond cellular systems embrace the disaggregation of Radio Access\nNetwork (RAN) components, exemplified by the evolution of the fronthual (FH)\nconnection between cellular baseband and radio unit equipment. Crucially,\nsynchronization over the FH is pivotal for reliable 5G services. In recent\nyears, there has been a push to move these links to an Ethernet-based packet\nnetwork topology, leveraging existing standards and ongoing research for\nTime-Sensitive Networking (TSN). However, TSN standards, such as Precision Time\nProtocol (PTP), focus on performance with little to no concern for security.\nThis increases the exposure of the open FH to security risks. Attacks targeting\nsynchronization mechanisms pose significant threats, potentially disrupting 5G\nnetworks and impairing connectivity.\n  In this paper, we demonstrate the impact of successful spoofing and replay\nattacks against PTP synchronization. We show how a spoofing attack is able to\ncause a production-ready O-RAN and 5G-compliant private cellular base station\nto catastrophically fail within 2 seconds of the attack, necessitating manual\nintervention to restore full network operations. To counter this, we design a\nMachine Learning (ML)-based monitoring solution capable of detecting various\nmalicious attacks with over 97.5% accuracy.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13049v1"}}, {"model": "App.cryptographyarticle", "pk": 53, "fields": {"title": "Are Data Experts Buying into Differentially Private Synthetic Data? Gathering Community Perspectives", "date": "2024-12-17", "authors": "Lucas Rosenblatt, Bill Howe, Julia Stoyanovich", "content": "Data privacy is a core tenet of responsible computing, and in the United\nStates, differential privacy (DP) is the dominant technical operationalization\nof privacy-preserving data analysis. With this study, we qualitatively examine\none class of DP mechanisms: private data synthesizers. To that end, we\nconducted semi-structured interviews with data experts: academics and\npractitioners who regularly work with data. Broadly, our findings suggest that\nquantitative DP benchmarks must be grounded in practitioner needs, while\ncommunication challenges persist. Participants expressed a need for\ncontext-aware DP solutions, focusing on parity between research outcomes on\nreal and synthetic data. Our analysis led to three recommendations: (1) improve\nexisting insufficient sanitized benchmarks; successful DP implementations\nrequire well-documented, partner-vetted use cases, (2) organizations using DP\nsynthetic data should publish discipline-specific standards of evidence, and\n(3) tiered data access models could allow researchers to gradually access\nsensitive data based on demonstrated competence with high-privacy, low-fidelity\nsynthetic data.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13030v1"}}, {"model": "App.cryptographyarticle", "pk": 54, "fields": {"title": "Queries, Representation & Detection: The Next 100 Model Fingerprinting Schemes", "date": "2024-12-17", "authors": "Augustin Godinot, Erwan Le Merrer, Camilla Penzo, François Taïani, Gilles Trédan", "content": "The deployment of machine learning models in operational contexts represents\na significant investment for any organisation. Consequently, the risk of these\nmodels being misappropriated by competitors needs to be addressed. In recent\nyears, numerous proposals have been put forth to detect instances of model\nstealing. However, these proposals operate under implicit and disparate data\nand model access assumptions; as a consequence, it remains unclear how they can\nbe effectively compared to one another. Our evaluation shows that a simple\nbaseline that we introduce performs on par with existing state-of-the-art\nfingerprints, which, on the other hand, are much more complex. To uncover the\nreasons behind this intriguing result, this paper introduces a systematic\napproach to both the creation of model fingerprinting schemes and their\nevaluation benchmarks. By dividing model fingerprinting into three core\ncomponents -- Query, Representation and Detection (QuRD) -- we are able to\nidentify $\\sim100$ previously unexplored QuRD combinations and gain insights\ninto their performance. Finally, we introduce a set of metrics to compare and\nguide the creation of more representative model stealing detection benchmarks.\nOur approach reveals the need for more challenging benchmarks and a sound\ncomparison with baselines. To foster the creation of new fingerprinting schemes\nand benchmarks, we open-source our fingerprinting toolbox.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.13021v1"}}, {"model": "App.cryptographyarticle", "pk": 55, "fields": {"title": "Liftability and Contracting Property of Multi-EGS Groups", "date": "2024-12-17", "authors": "Arsalan Akram Malik, Dmytro Savchuk", "content": "We provide sufficient conditions for the multi-EGS groups to be liftable and\nthus produce new examples of groups acting transitively on regular trees of\nfinite degree stabilizing one of the ends, whose closures are scale groups as\ndefined by Willis. Additionally, we explicitly compute the contracting nuclei\nof the groups in this class. We also specialize our results to the classes of\nmulti-edge spinal group and EGS-groups.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12915v1"}}, {"model": "App.cryptographyarticle", "pk": 56, "fields": {"title": "Evaluating tamper resistance of digital forensic artifacts during event reconstruction", "date": "2024-12-17", "authors": "Céline Vanini, Chris Hargreaves, Frank Breitinger", "content": "Event reconstruction is a fundamental part of the digital forensic process,\nhelping to answer key questions like who, what, when, and how. A common way of\naccomplishing that is to use tools to create timelines, which are then\nanalyzed. However, various challenges exist, such as large volumes of data or\ncontamination. While prior research has focused on simplifying timelines, less\nattention has been given to tampering, i.e., the deliberate manipulation of\nevidence, which can lead to errors in interpretation. This article addresses\nthe issue by proposing a framework to assess the tamper resistance of data\nsources used in event reconstruction. We discuss factors affecting data\nresilience, introduce a scoring system for evaluation, and illustrate its\napplication with case studies. This work aims to improve the reliability of\nforensic event reconstruction by considering tamper resistance.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12814v1"}}, {"model": "App.cryptographyarticle", "pk": 57, "fields": {"title": "RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service", "date": "2024-12-17", "authors": "Yihang Cheng, Lan Zhang, Junyang Wang, Mu Yuan, Yunhao Yao", "content": "Retrieval-augmented generation (RAG) improves the service quality of large\nlanguage models by retrieving relevant documents from credible literature and\nintegrating them into the context of the user query. Recently, the rise of the\ncloud RAG service has made it possible for users to query relevant documents\nconveniently. However, directly sending queries to the cloud brings potential\nprivacy leakage. In this paper, we are the first to formally define the\nprivacy-preserving cloud RAG service to protect the user query and propose\nRemoteRAG as a solution regarding privacy, efficiency, and accuracy. For\nprivacy, we introduce $(n,\\epsilon)$-DistanceDP to characterize privacy leakage\nof the user query and the leakage inferred from relevant documents. For\nefficiency, we limit the search range from the total documents to a small\nnumber of selected documents related to a perturbed embedding generated from\n$(n,\\epsilon)$-DistanceDP, so that computation and communication costs required\nfor privacy protection significantly decrease. For accuracy, we ensure that the\nsmall range includes target documents related to the user query with detailed\ntheoretical analysis. Experimental results also demonstrate that RemoteRAG can\nresist existing embedding inversion attack methods while achieving no loss in\nretrieval under various settings. Moreover, RemoteRAG is efficient, incurring\nonly $0.67$ seconds and $46.66$KB of data transmission ($2.72$ hours and $1.43$\nGB with the non-optimized privacy-preserving scheme) when retrieving from a\ntotal of $10^6$ documents.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12775v1"}}, {"model": "App.cryptographyarticle", "pk": 58, "fields": {"title": "EmbedFuzz: High Speed Fuzzing Through Transplantation", "date": "2024-12-17", "authors": "Florian Hofhammer, Qinying Wang, Atri Bhattacharyya, Majid Salehi, Bruno Crispo, Manuel Egele, Mathias Payer, Marcel Busch", "content": "Dynamic analysis and especially fuzzing are challenging tasks for embedded\nfirmware running on modern low-end Microcontroller Units (MCUs) due to\nperformance overheads from instruction emulation, the difficulty of emulating\nthe vast space of available peripherals, and low availability of open-source\nembedded firmware. Consequently, efficient security testing of MCU firmware has\nproved to be a resource- and engineering-heavy endeavor.\n  EmbedFuzz introduces an efficient end-to-end fuzzing framework for MCU\nfirmware. Our novel firmware transplantation technique converts binary MCU\nfirmware to a functionally equivalent and fuzzing-enhanced version of the\nfirmware which executes on a compatible high-end device at native performance.\nBesides the performance gains, our system enables advanced introspection\ncapabilities based on tooling for typical Linux user space processes, thus\nsimplifying analysis of crashes and bug triaging. In our evaluation against\nstate-of-the-art MCU fuzzers, EmbedFuzz exhibits up to eight-fold fuzzing\nthroughput while consuming at most a fourth of the energy thanks to its native\nexecution.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12746v1"}}, {"model": "App.cryptographyarticle", "pk": 59, "fields": {"title": "Automated Penetration Testing: Formalization and Realization", "date": "2024-12-17", "authors": "Charilaos Skandylas, Mikael Asplund", "content": "Recent changes in standards and regulations, driven by the increasing\nimportance of software systems in meeting societal needs, mandate increased\nsecurity testing of software systems. Penetration testing has been shown to be\na reliable method to asses software system security. However, manual\npenetration testing is labor-intensive and requires highly skilled\npractitioners. Given the shortage of cybersecurity experts and current societal\nneeds, increasing the degree of automation involved in penetration testing can\naid in fulfilling the demands for increased security testing. In this work, we\nformally express the penetration testing problem at the architectural level and\nsuggest a general self-organizing architecture that can be instantiated to\nautomate penetration testing of real systems. We further describe and implement\na specialization of the architecture in the ADAPT tool, targeting systems\ncomposed of hosts and services. We evaluate and demonstrate the feasibility of\nADAPT by automatically performing penetration tests with success against:\nMetasploitable2, Metasploitable3, and a realistic virtual network used as a lab\nenvironment for penetration tester training.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12745v1"}}, {"model": "App.cryptographyarticle", "pk": 60, "fields": {"title": "Deep Learning for Resilient Adversarial Decision Fusion in Byzantine Networks", "date": "2024-12-17", "authors": "Kassem Kallas", "content": "This paper introduces a deep learning-based framework for resilient decision\nfusion in adversarial multi-sensor networks, providing a unified mathematical\nsetup that encompasses diverse scenarios, including varying Byzantine node\nproportions, synchronized and unsynchronized attacks, unbalanced priors,\nadaptive strategies, and Markovian states. Unlike traditional methods, which\ndepend on explicit parameter tuning and are limited by scenario-specific\nassumptions, the proposed approach employs a deep neural network trained on a\nglobally constructed dataset to generalize across all cases without requiring\nadaptation. Extensive simulations validate the method's robustness, achieving\nsuperior accuracy, minimal error probability, and scalability compared to\nstate-of-the-art techniques, while ensuring computational efficiency for\nreal-time applications. This unified framework demonstrates the potential of\ndeep learning to revolutionize decision fusion by addressing the challenges\nposed by Byzantine nodes in dynamic adversarial environments.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12739v1"}}, {"model": "App.cryptographyarticle", "pk": 61, "fields": {"title": "AsyncSC: An Asynchronous Sidechain for Multi-Domain Data Exchange in Internet of Things", "date": "2024-12-17", "authors": "Lingxiao Yang, Xuewen Dong, Zhiguo Wan, Sheng Gao, Wei Tong, Di Lu, Yulong Shen, Xiaojiang Du", "content": "Sidechain techniques improve blockchain scalability and interoperability,\nproviding decentralized exchange and cross-chain collaboration solutions for\nInternet of Things (IoT) data across various domains. However, current\nstate-of-the-art (SOTA) schemes for IoT multi-domain data exchange are\nconstrained by the need for synchronous networks, hindering efficient\ncross-chain interactions in discontinuous networks and leading to suboptimal\ndata exchange. In this paper, we propose AsyncSC, a novel asynchronous\nsidechain construction. It employs a committee to provide Cross-Blockchain as a\nService (C-BaaS) for data exchange in multi-domain IoT. To fulfill the need for\nasynchronous and efficient data exchange, we combine the ideas of aggregate\nsignatures and verifiable delay functions to devise a novel cryptographic\nprimitive called delayed aggregate signature (DAS), which constructs\nasynchronous cross-chain proofs (ACPs) that ensure the security of cross-chain\ninteractions. To ensure the consistency of asynchronous transactions, we\npropose a multilevel buffered transaction pool that guarantees the transaction\nsequencing. We analyze and prove the security of AsyncSC, simulate an\nasynchronous communication environment, and conduct a comprehensive evaluation.\nThe results show that AsyncSC outperforms SOTA schemes, improving throughput by\nan average of 1.21 to 3.96 times, reducing transaction latency by 59.76% to\n83.61%, and maintaining comparable resource overhead.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12723v1"}}, {"model": "App.cryptographyarticle", "pk": 62, "fields": {"title": "Defending LVLMs Against Vision Attacks through Partial-Perception Supervision", "date": "2024-12-17", "authors": "Qi Zhou, Tianlin Li, Qing Guo, Dongxia Wang, Yun Lin, Yang Liu, Jin Song Dong", "content": "Recent studies have raised significant concerns regarding the vulnerability\nof Large Vision Language Models (LVLMs) to maliciously injected or perturbed\ninput images, which can mislead their responses. Existing defense methods show\nthat such vision attacks are sensitive to image modifications especially\ncropping, using majority voting across responses of modified images as\ncorrected responses. However, these modifications often result in partial\nimages and distort the semantics, which reduces response quality on clean\nimages after voting. Instead of directly using responses from partial images\nfor voting, we investigate using them to supervise the LVLM's responses to the\noriginal images. We propose a black-box, training-free method called DPS\n(Defense through Partial-Perception Supervision). In this approach, the model\nis prompted using the responses generated by a model that perceives only a\npartial image. With DPS, the model can adjust its response based on partial\nimage understanding when under attack, while confidently maintaining its\noriginal response for clean input. Our findings show that the weak model can\nsupervise the strong model: when faced with an attacked input, the strong model\nbecomes less confident and adjusts its response based on the weak model's\npartial understanding, effectively defending against the attack. With clean\ninput, it confidently maintains its original response. Empirical experiments\nshow our method outperforms the baseline, cutting the average attack success\nrate by 76.3% across six datasets on three popular models.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12722v1"}}, {"model": "App.cryptographyarticle", "pk": 63, "fields": {"title": "Exploring AI-Enabled Cybersecurity Frameworks: Deep-Learning Techniques, GPU Support, and Future Enhancements", "date": "2024-12-17", "authors": "Tobias Becher, Simon Torka", "content": "Traditional rule-based cybersecurity systems have proven highly effective\nagainst known malware threats. However, they face challenges in detecting novel\nthreats. To address this issue, emerging cybersecurity systems are\nincorporating AI techniques, specifically deep-learning algorithms, to enhance\ntheir ability to detect incidents, analyze alerts, and respond to events. While\nthese techniques offer a promising approach to combating dynamic security\nthreats, they often require significant computational resources. Therefore,\nframeworks that incorporate AI-based cybersecurity mechanisms need to support\nthe use of GPUs to ensure optimal performance.\n  Many cybersecurity framework vendors do not provide sufficiently detailed\ninformation about their implementation, making it difficult to assess the\ntechniques employed and their effectiveness. This study aims to overcome this\nlimitation by providing an overview of the most used cybersecurity frameworks\nthat utilize AI techniques, specifically focusing on frameworks that provide\ncomprehensive information about their implementation. Our primary objective is\nto identify the deep-learning techniques employed by these frameworks and\nevaluate their support for GPU acceleration. We have identified a total of\n\\emph{two} deep-learning algorithms that are utilized by \\emph{three} out of 38\nselected cybersecurity frameworks. Our findings aim to assist in selecting\nopen-source cybersecurity frameworks for future research and assessing any\ndiscrepancies between deep-learning techniques used in theory and practice.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12648v1"}}, {"model": "App.cryptographyarticle", "pk": 64, "fields": {"title": "Building Gradient Bridges: Label Leakage from Restricted Gradient Sharing in Federated Learning", "date": "2024-12-17", "authors": "Rui Zhang, Ka-Ho Chow, Ping Li", "content": "The growing concern over data privacy, the benefits of utilizing data from\ndiverse sources for model training, and the proliferation of networked devices\nwith enhanced computational capabilities have all contributed to the rise of\nfederated learning (FL). The clients in FL collaborate to train a global model\nby uploading gradients computed on their private datasets without collecting\nraw data. However, a new attack surface has emerged from gradient sharing,\nwhere adversaries can restore the label distribution of a victim's private data\nby analyzing the obtained gradients. To mitigate this privacy leakage, existing\nlightweight defenses restrict the sharing of gradients, such as encrypting the\nfinal-layer gradients or locally updating the parameters within. In this paper,\nwe introduce a novel attack called Gradient Bridge (GDBR) that recovers the\nlabel distribution of training data from the limited gradient information\nshared in FL. GDBR explores the relationship between the layer-wise gradients,\ntracks the flow of gradients, and analytically derives the batch training\nlabels. Extensive experiments show that GDBR can accurately recover more than\n80% of labels in various FL settings. GDBR highlights the inadequacy of\nrestricted gradient sharing-based defenses and calls for the design of\neffective defense schemes in FL.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12640v1"}}, {"model": "App.cryptographyarticle", "pk": 65, "fields": {"title": "Improving the Transferability of 3D Point Cloud Attack via Spectral-aware Admix and Optimization Designs", "date": "2024-12-17", "authors": "Shiyu Hu, Daizong Liu, Wei Hu", "content": "Deep learning models for point clouds have shown to be vulnerable to\nadversarial attacks, which have received increasing attention in various\nsafety-critical applications such as autonomous driving, robotics, and\nsurveillance. Existing 3D attackers generally design various attack strategies\nin the white-box setting, requiring the prior knowledge of 3D model details.\nHowever, real-world 3D applications are in the black-box setting, where we can\nonly acquire the outputs of the target classifier. Although few recent works\ntry to explore the black-box attack, they still achieve limited attack success\nrates (ASR). To alleviate this issue, this paper focuses on attacking the 3D\nmodels in a transfer-based black-box setting, where we first carefully design\nadversarial examples in a white-box surrogate model and then transfer them to\nattack other black-box victim models. Specifically, we propose a novel\nSpectral-aware Admix with Augmented Optimization method (SAAO) to improve the\nadversarial transferability. In particular, since traditional Admix strategy\nare deployed in the 2D domain that adds pixel-wise images for perturbing, we\ncan not directly follow it to merge point clouds in coordinate domain as it\nwill destroy the geometric shapes. Therefore, we design spectral-aware fusion\nthat performs Graph Fourier Transform (GFT) to get spectral features of the\npoint clouds and add them in the spectral domain. Afterward, we run a few steps\nwith spectral-aware weighted Admix to select better optimization paths as well\nas to adjust corresponding learning weights. At last, we run more steps to\ngenerate adversarial spectral feature along the optimization path and perform\nInverse-GFT on the adversarial spectral feature to obtain the adversarial\nexample in the data domain. Experiments show that our SAAO achieves better\ntransferability compared to existing 3D attack methods.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12626v1"}}, {"model": "App.cryptographyarticle", "pk": 66, "fields": {"title": "if-ZKP: Intel FPGA-Based Acceleration of Zero Knowledge Proofs", "date": "2024-12-17", "authors": "Shahzad Ahmad Butt, Benjamin Reynolds, Veeraraghavan Ramamurthy, Xiao Xiao, Pohrong Chu, Setareh Sharifian, Sergey Gribok, Bogdan Pasca", "content": "Zero-Knowledge Proofs (ZKPs) have emerged as an important cryptographic\ntechnique allowing one party (prover) to prove the correctness of a statement\nto some other party (verifier) and nothing else. ZKPs give rise to user's\nprivacy in many applications such as blockchains, digital voting, and machine\nlearning. Traditionally, ZKPs suffered from poor scalability but recently, a\nsub-class of ZKPs known as Zero-knowledge Succinct Non-interactive ARgument of\nKnowledges (zk-SNARKs) have addressed this challenge. They are getting\nsignificant attention and are being implemented by many public libraries. In\nthis paper, we present a novel scalable architecture that is suitable for\naccelerating the zk-SNARK prover compute on FPGAs. We focus on the multi-scalar\nmultiplication (MSM) that accounts for the majority of computation time spent\nin zk-SNARK systems. The MSM calculations extensive rely on modular arithmetic\nso highly optimized Intel IP Libraries for modular arithmetic are used. The\nproposed architecture exploits the parallelism inherent to MSM and is\nimplemented using the Intel OneAPI framework for FPGAs. Our implementation runs\n110x-150x faster compared to reference software library, uses a generic curve\nform in Jacobian coordinates and is the first to report FPGA hardware\nacceleration results for BLS12-381 and BN128 family of elliptic curves.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12481v1"}}, {"model": "App.cryptographyarticle", "pk": 67, "fields": {"title": "Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script", "date": "2024-12-17", "authors": "Xi Cao, Yuan Sun, Jiajun Li, Quzong Gesang, Nuo Qun, Tashi Nyima", "content": "DNN-based language models perform excellently on various tasks, but even SOTA\nLLMs are susceptible to textual adversarial attacks. Adversarial texts play\ncrucial roles in multiple subfields of NLP. However, current research has the\nfollowing issues. (1) Most textual adversarial attack methods target\nrich-resourced languages. How do we generate adversarial texts for less-studied\nlanguages? (2) Most textual adversarial attack methods are prone to generating\ninvalid or ambiguous adversarial texts. How do we construct high-quality\nadversarial robustness benchmarks? (3) New language models may be immune to\npart of previously generated adversarial texts. How do we update adversarial\nrobustness benchmarks? To address the above issues, we introduce HITL-GAT, a\nsystem based on a general approach to human-in-the-loop generation of\nadversarial texts. HITL-GAT contains four stages in one pipeline: victim model\nconstruction, adversarial example generation, high-quality benchmark\nconstruction, and adversarial robustness evaluation. Additionally, we utilize\nHITL-GAT to make a case study on Tibetan script which can be a reference for\nthe adversarial research of other less-studied languages.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12478v1"}}, {"model": "App.cryptographyarticle", "pk": 68, "fields": {"title": "Characterizing the Networks Sending Enterprise Phishing Emails", "date": "2024-12-16", "authors": "Elisa Luo, Liane Young, Grant Ho, M. H. Afifi, Marco Schweighauser, Ethan Katz-Bassett, Asaf Cidon", "content": "Phishing attacks on enterprise employees present one of the most costly and\npotent threats to organizations. We explore an understudied facet of enterprise\nphishing attacks: the email relay infrastructure behind successfully delivered\nphishing emails. We draw on a dataset spanning one year across thousands of\nenterprises, billions of emails, and over 800,000 delivered phishing attacks.\nOur work sheds light on the network origins of phishing emails received by\nreal-world enterprises, differences in email traffic we observe from networks\nsending phishing emails, and how these characteristics change over time.\n  Surprisingly, we find that over one-third of the phishing email in our\ndataset originates from highly reputable networks, including Amazon and\nMicrosoft. Their total volume of phishing email is consistently high across\nmultiple months in our dataset, even though the overwhelming majority of email\nsent by these networks is benign. In contrast, we observe that a large portion\nof phishing emails originate from networks where the vast majority of emails\nthey send are phishing, but their email traffic is not consistent over time.\nTaken together, our results explain why no singular defense strategy, such as\nstatic blocklists (which are commonly used in email security filters deployed\nby organizations in our dataset), is effective at blocking enterprise phishing.\nBased on our offline analysis, we partnered with a large email security company\nto deploy a classifier that uses dynamically updated network-based features. In\na production environment over a period of 4.5 months, our new detector was able\nto identify 3-5% more enterprise email attacks that were previously undetected\nby the company's existing classifiers.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12403v1"}}, {"model": "App.cryptographyarticle", "pk": 69, "fields": {"title": "Privacy in Metalearning and Multitask Learning: Modeling and Separations", "date": "2024-12-16", "authors": "Maryam Aliakbarpour, Konstantina Bairaktari, Adam Smith, Marika Swanberg, Jonathan Ullman", "content": "Model personalization allows a set of individuals, each facing a different\nlearning task, to train models that are more accurate for each person than\nthose they could develop individually. The goals of personalization are\ncaptured in a variety of formal frameworks, such as multitask learning and\nmetalearning. Combining data for model personalization poses risks for privacy\nbecause the output of an individual's model can depend on the data of other\nindividuals. In this work we undertake a systematic study of differentially\nprivate personalized learning. Our first main contribution is to construct a\ntaxonomy of formal frameworks for private personalized learning. This taxonomy\ncaptures different formal frameworks for learning as well as different threat\nmodels for the attacker. Our second main contribution is to prove separations\nbetween the personalized learning problems corresponding to different choices.\nIn particular, we prove a novel separation between private multitask learning\nand private metalearning.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12374v1"}}, {"model": "App.cryptographyarticle", "pk": 70, "fields": {"title": "Quantum Adversarial Machine Learning and Defense Strategies: Challenges and Opportunities", "date": "2024-12-16", "authors": "Eric Yocam, Anthony Rizi, Mahesh Kamepalli, Varghese Vaidyan, Yong Wang, Gurcan Comert", "content": "As quantum computing continues to advance, the development of quantum-secure\nneural networks is crucial to prevent adversarial attacks. This paper proposes\nthree quantum-secure design principles: (1) using post-quantum cryptography,\n(2) employing quantum-resistant neural network architectures, and (3) ensuring\ntransparent and accountable development and deployment. These principles are\nsupported by various quantum strategies, including quantum data anonymization,\nquantum-resistant neural networks, and quantum encryption. The paper also\nidentifies open issues in quantum security, privacy, and trust, and recommends\nexploring adaptive adversarial attacks and auto adversarial attacks as future\ndirections. The proposed design principles and recommendations provide guidance\nfor developing quantum-secure neural networks, ensuring the integrity and\nreliability of machine learning models in the quantum era.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12373v1"}}, {"model": "App.cryptographyarticle", "pk": 71, "fields": {"title": "Scam Detection for Ethereum Smart Contracts: Leveraging Graph Representation Learning for Secure Blockchain", "date": "2024-12-16", "authors": "Yihong Jin, Ze Yang", "content": "The detection of scams within Ethereum smart contracts is a critical\nchallenge due to their increasing exploitation for fraudulent activities,\nleading to significant financial and reputational damages. Existing detection\nmethods often rely on contract code analysis or manually extracted features,\nwhich suffer from scalability and adaptability limitations. In this study, we\nintroduce an innovative method that leverages graph representation learning to\nexamine transaction patterns and identify fraudulent contracts. By transforming\nEthereum transaction data into graph structures and employing advanced machine\nlearning models, we achieve robust classification performance. Our method\naddresses label imbalance through SMOTE-ENN techniques and evaluates models\nlike Multi-Layer Perceptron (MLP) and Graph Convolutional Networks (GCN).\nExperimental results indicate that the MLP model surpasses the GCN in this\ncontext, with real-world evaluations aligning closely with domain-specific\nanalyses. This study provides a scalable and effective solution for enhancing\ntrust and security in the Ethereum ecosystem.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12370v1"}}, {"model": "App.cryptographyarticle", "pk": 72, "fields": {"title": "F-RBA: A Federated Learning-based Framework for Risk-based Authentication", "date": "2024-12-16", "authors": "Hamidreza Fereidouni, Abdelhakim Senhaji Hafid, Dimitrios Makrakis, Yaser Baseri", "content": "The proliferation of Internet services has led to an increasing need to\nprotect private data. User authentication serves as a crucial mechanism to\nensure data security. Although robust authentication forms the cornerstone of\nremote service security, it can still leave users vulnerable to credential\ndisclosure, device-theft attacks, session hijacking, and inadequate adaptive\nsecurity measures. Risk-based Authentication (RBA) emerges as a potential\nsolution, offering a multi-level authentication approach that enhances user\nexperience without compromising security. In this paper, we propose a Federated\nRisk-based Authentication (F-RBA) framework that leverages Federated Learning\nto ensure privacy-centric training, keeping user data local while distributing\nlearning across devices. Whereas traditional approaches rely on centralized\nstorage, F-RBA introduces a distributed architecture where risk assessment\noccurs locally on users' devices. The framework's core innovation lies in its\nsimilarity-based feature engineering approach, which addresses the\nheterogeneous data challenges inherent in federated settings, a significant\nadvancement for distributed authentication. By facilitating real-time risk\nevaluation across devices while maintaining unified user profiles, F-RBA\nachieves a balance between data protection, security, and scalability. Through\nits federated approach, F-RBA addresses the cold-start challenge in risk model\ncreation, enabling swift adaptation to new users without compromising security.\nEmpirical evaluation using a real-world multi-user dataset demonstrates the\nframework's effectiveness, achieving a superior true positive rate for\ndetecting suspicious logins compared to conventional unsupervised anomaly\ndetection models. This research introduces a new paradigm for privacy-focused\nRBA in distributed digital environments, facilitating advancements in federated\nsecurity systems.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12324v1"}}, {"model": "App.cryptographyarticle", "pk": 73, "fields": {"title": "Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection", "date": "2024-12-16", "authors": "Ira Ceka, Feitong Qiao, Anik Dey, Aastha Valechia, Gail Kaiser, Baishakhi Ray", "content": "Despite their remarkable success, large language models (LLMs) have shown\nlimited ability on applied tasks such as vulnerability detection. We\ninvestigate various prompting strategies for vulnerability detection and, as\npart of this exploration, propose a prompting strategy that integrates natural\nlanguage descriptions of vulnerabilities with a contrastive chain-of-thought\nreasoning approach, augmented using contrastive samples from a synthetic\ndataset. Our study highlights the potential of LLMs to detect vulnerabilities\nby integrating natural language descriptions, contrastive reasoning, and\nsynthetic examples into a comprehensive prompting framework. Our results show\nthat this approach can enhance LLM understanding of vulnerabilities. On a\nhigh-quality vulnerability detection dataset such as SVEN, our prompting\nstrategies can improve accuracies, F1-scores, and pairwise accuracies by 23%,\n11%, and 14%, respectively.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12039v1"}}, {"model": "App.cryptographyarticle", "pk": 74, "fields": {"title": "Codes from $A_m$-invariant polynomials", "date": "2024-12-16", "authors": "Giacomo Micheli, Vincenzo Pallozzi Lavorante, Phillip Waitkevich", "content": "Let $q$ be a prime power. This paper provides a new class of linear codes\nthat arises from the action of the alternating group on $\\mathbb\nF_q[x_1,\\dots,x_m]$ combined with the ideas in (M. Datta and T. Johnsen, 2022).\nCompared with Generalized Reed-Muller codes with similar parameters, our codes\nhave the same asymptotic relative distance but a better rate. Our results\nfollow from combinations of Galois theoretical methods with Weil-type bounds\nfor the number of points of hypersurfaces over finite fields.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12005v1"}}, {"model": "App.cryptographyarticle", "pk": 75, "fields": {"title": "Efficient Layered New Bit-Flipping QC-MDPC Decoder for BIKE Post-Quantum Cryptography", "date": "2024-12-16", "authors": "Jiaxuan Cai, Xinmiao Zhang", "content": "The medium-density parity-check (MDPC) code-based Bit Flipping Key\nEncapsulation (BIKE) mechanism remains a candidate of post-quantum cryptography\nstandardization. The latest version utilizes a new bit-flipping (BF) decoding\nalgorithm, which decides the BF threshold by an affine function with\nhigh-precision coefficients. Previous BF decoder implementations can be\nextended to the new algorithm. However, they suffer from large memories that\ndominate the overall complexity. This paper proposes a column-layered decoder\nfor the new BIKE BF decoding algorithm to substantially reduce the memory\nrequirement, and optimizes the affine BF threshold function coefficients to\nreduce the code length needed for the same security level. For the first time,\nour work also investigates the impact of finite precision representation of the\nthreshold coefficients on the decoding performance. For an example MDPC code\nconsidered for the standard, the proposed layered BF decoder achieves 20%\ncomplexity reduction compared to the best prior effort with a very small\nlatency overhead.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11997v1"}}, {"model": "App.cryptographyarticle", "pk": 76, "fields": {"title": "But Can You Use It? Design Recommendations for Differentially Private Interactive Systems", "date": "2024-12-16", "authors": "Liudas Panavas, Joshua Snoke, Erika Tyagi, Claire McKay Bowen, Aaron R. Williams", "content": "Accessing data collected by federal statistical agencies is essential for\npublic policy research and improving evidence-based decision making, such as\nevaluating the effectiveness of social programs, understanding demographic\nshifts, or addressing public health challenges. Differentially private\ninteractive systems, or validation servers, can form a crucial part of the\ndata-sharing infrastructure. They may allow researchers to query targeted\nstatistics, providing flexible, efficient access to specific insights, reducing\nthe need for broad data releases and supporting timely, focused research.\nHowever, they have not yet been practically implemented. While substantial\ntheoretical work has been conducted on the privacy and accuracy guarantees of\ndifferentially private mechanisms, prior efforts have not considered usability\nas an explicit goal of interactive systems. This work outlines and considers\nthe barriers to developing differentially private interactive systems for\ninforming public policy and offers an alternative way forward. We propose\nbalancing three design considerations: privacy assurance, statistical utility,\nand system usability, we develop recommendations for making differentially\nprivate interactive systems work in practice, we present an example\narchitecture based on these recommendations, and we provide an outline of how\nto conduct the necessary user-testing. Our work seeks to move the practical\ndevelopment of differentially private interactive systems forward to better aid\npublic policy making and spark future research.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11794v1"}}, {"model": "App.cryptographyarticle", "pk": 77, "fields": {"title": "Efficiently Achieving Secure Model Training and Secure Aggregation to Ensure Bidirectional Privacy-Preservation in Federated Learning", "date": "2024-12-16", "authors": "Xue Yang, Depan Peng, Yan Feng, Xiaohu Tang, Weijun Fang, Jun Shao", "content": "Bidirectional privacy-preservation federated learning is crucial as both\nlocal gradients and the global model may leak privacy. However, only a few\nworks attempt to achieve it, and they often face challenges such as excessive\ncommunication and computational overheads, or significant degradation of model\naccuracy, which hinders their practical applications. In this paper, we design\nan efficient and high-accuracy bidirectional privacy-preserving scheme for\nfederated learning to complete secure model training and secure aggregation. To\nefficiently achieve bidirectional privacy, we design an efficient and\naccuracy-lossless model perturbation method on the server side (called\n$\\mathbf{MP\\_Server}$) that can be combined with local differential privacy\n(LDP) to prevent clients from accessing the model, while ensuring that the\nlocal gradients obtained on the server side satisfy LDP. Furthermore, to ensure\nmodel accuracy, we customize a distributed differential privacy mechanism on\nthe client side (called $\\mathbf{DDP\\_Client}$). When combined with\n$\\mathbf{MP\\_Server}$, it ensures LDP of the local gradients, while ensuring\nthat the aggregated result matches the accuracy of central differential privacy\n(CDP). Extensive experiments demonstrate that our scheme significantly\noutperforms state-of-the-art bidirectional privacy-preservation baselines\n(SOTAs) in terms of computational cost, model accuracy, and defense ability\nagainst privacy attacks. Particularly, given target accuracy, the training time\nof SOTAs is approximately $200$ times, or even over $1000$ times, longer than\nthat of our scheme. When the privacy budget is set relatively small, our scheme\nincurs less than $6\\%$ accuracy loss compared to the privacy-ignoring method,\nwhile SOTAs suffer up to $20\\%$ accuracy loss. Experimental results also show\nthat the defense capability of our scheme outperforms than SOTAs.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11737v1"}}, {"model": "App.cryptographyarticle", "pk": 78, "fields": {"title": "On Large Language Models in Mission-Critical IT Governance: Are We Ready Yet?", "date": "2024-12-16", "authors": "Matteo Esposito, Francesco Palagiano, Valentina Lenarduzzi, Davide Taibi", "content": "Context. The security of critical infrastructure has been a fundamental\nconcern since the advent of computers, and this concern has only intensified in\ntoday's cyber warfare landscape. Protecting mission-critical systems (MCSs),\nincluding essential assets like healthcare, telecommunications, and military\ncoordination, is vital for national security. These systems require prompt and\ncomprehensive governance to ensure their resilience, yet recent events have\nshown that meeting these demands is increasingly challenging. Aim. Building on\nprior research that demonstrated the potential of GAI, particularly Large\nLanguage Models (LLMs), in improving risk analysis tasks, we aim to explore\npractitioners' perspectives, specifically developers and security personnel, on\nusing generative AI (GAI) in the governance of IT MCSs seeking to provide\ninsights and recommendations for various stakeholders, including researchers,\npractitioners, and policymakers. Method. We designed a survey to collect\npractical experiences, concerns, and expectations of practitioners who develop\nand implement security solutions in the context of MCSs. Analyzing this data\nwill help identify key trends, challenges, and opportunities for introducing\nGAIs in this niche domain. Conclusions and Future Works. Our findings highlight\nthat the safe use of LLMs in MCS governance requires interdisciplinary\ncollaboration. Researchers should focus on designing regulation-oriented models\nand focus on accountability; practitioners emphasize data protection and\ntransparency, while policymakers must establish a unified AI framework with\nglobal benchmarks to ensure ethical and secure LLMs-based MCS governance.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11698v1"}}, {"model": "App.cryptographyarticle", "pk": 79, "fields": {"title": "Just a Simple Transformation is Enough for Data Protection in Vertical Federated Learning", "date": "2024-12-16", "authors": "Andrei Semenov, Philip Zmushko, Alexander Pichugin, Aleksandr Beznosikov", "content": "Vertical Federated Learning (VFL) aims to enable collaborative training of\ndeep learning models while maintaining privacy protection. However, the VFL\nprocedure still has components that are vulnerable to attacks by malicious\nparties. In our work, we consider feature reconstruction attacks, a common risk\ntargeting input data compromise. We theoretically claim that feature\nreconstruction attacks cannot succeed without knowledge of the prior\ndistribution on data. Consequently, we demonstrate that even simple model\narchitecture transformations can significantly impact the protection of input\ndata during VFL. Confirming these findings with experimental results, we show\nthat MLP-based models are resistant to state-of-the-art feature reconstruction\nattacks.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11689v1"}}, {"model": "App.cryptographyarticle", "pk": 80, "fields": {"title": "SeSeMI: Secure Serverless Model Inference on Sensitive Data", "date": "2024-12-16", "authors": "Guoyu Hu, Yuncheng Wu, Gang Chen, Tien Tuan Anh Dinh, Beng Chin Ooi", "content": "Model inference systems are essential for implementing end-to-end data\nanalytics pipelines that deliver the benefits of machine learning models to\nusers. Existing cloud-based model inference systems are costly, not easy to\nscale, and must be trusted in handling the models and user request data.\nServerless computing presents a new opportunity, as it provides elasticity and\nfine-grained pricing. Our goal is to design a serverless model inference system\nthat protects models and user request data from untrusted cloud providers. It\noffers high performance and low cost, while requiring no intrusive changes to\nthe current serverless platforms. To realize our goal, we leverage trusted\nhardware. We identify and address three challenges in using trusted hardware\nfor serverless model inference. These challenges arise from the high-level\nabstraction of serverless computing, the performance overhead of trusted\nhardware, and the characteristics of model inference workloads. We present\nSeSeMI, a secure, efficient, and cost-effective serverless model inference\nsystem. It adds three novel features non-intrusively to the existing serverless\ninfrastructure and nothing else.The first feature is a key service that\nestablishes secure channels between the user and the serverless instances,\nwhich also provides access control to models and users' data. The second is an\nenclave runtime that allows one enclave to process multiple concurrent\nrequests. The final feature is a model packer that allows multiple models to be\nexecuted by one serverless instance. We build SeSeMI on top of Apache\nOpenWhisk, and conduct extensive experiments with three popular machine\nlearning models. The results show that SeSeMI achieves low latency and low cost\nat scale for realistic workloads.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11640v1"}}, {"model": "App.cryptographyarticle", "pk": 81, "fields": {"title": "DB-PAISA: Discovery-Based Privacy-Agile IoT Sensing+Actuation", "date": "2024-12-16", "authors": "Isita Bagayatkar, Youngil Kim, Gene Tsudik", "content": "Internet of Things (IoT) devices are becoming increasingly commonplace in\nnumerous public and semi-private settings. Currently, most such devices lack\nmechanisms to facilitate their discovery by casual (nearby) users who are not\nowners or operators. However, these users are potentially being sensed, and/or\nactuated upon, by these devices, without their knowledge or consent. This\nnaturally triggers privacy, security, and safety issues.\n  To address this problem, some recent work explored device transparency in the\nIoT ecosystem. The intuitive approach is for each device to periodically and\nsecurely broadcast (announce) its presence and capabilities to all nearby\nusers. While effective, when no new users are present, this push-based approach\ngenerates a substantial amount of unnecessary network traffic and needlessly\ninterferes with normal device operation.\n  In this work, we construct DB-PAISA which addresses these issues via a\npull-based method, whereby devices reveal their presence and capabilities only\nupon explicit user request. Each device guarantees a secure timely response\n(even if fully compromised by malware) based on a small active Root-of-Trust\n(RoT). DB-PAISA requires no hardware modifications and is suitable for a range\nof current IoT devices. To demonstrate its feasibility and practicality, we\nbuilt a fully functional and publicly available prototype. It is implemented\natop a commodity MCU (NXP LCP55S69) and operates in tandem with a\nsmartphone-based app. Using this prototype, we evaluate energy consumption and\nother performance factors.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11572v1"}}, {"model": "App.cryptographyarticle", "pk": 82, "fields": {"title": "OTA-Key: Over the Air Key Management for Flexible and Reliable IoT Device Provision", "date": "2024-12-16", "authors": "Qian Zhang, Yi He, Yue Xiao, Xiaoli Zhang, Chunhua Song", "content": "As the Internet of Things (IoT) industry advances, the imperative to secure\nIoT devices has become increasingly critical. Current practices in both\nindustry and academia advocate for the enhancement of device security through\nkey installation. However, it has been observed that, in practice, IoT vendors\nfrequently assign shared keys to batches of devices. This practice can expose\ndevices to risks, such as data theft by attackers or large-scale Distributed\nDenial of Service (DDoS) attacks. To address this issue, our intuition is to\nassign a unique key to each device. Unfortunately, this strategy proves to be\nhighly complex within the IoT context, as existing keys are typically hardcoded\ninto the firmware, necessitating the creation of bespoke firmware for each\ndevice. Furthermore, correct pairing of device keys with their respective\ndevices is crucial. Errors in this pairing process would incur substantial\nhuman and temporal resources to rectify and require extensive communication\nbetween IoT vendors, device manufacturers, and cloud platforms, leading to\nsignificant communication overhead. To overcome these challenges, we propose\nthe OTA-Key scheme. This approach fundamentally decouples device keys from the\nfirmware features stored in flash memory, utilizing an intermediary server to\nallocate unique device keys in two distinct stages and update keys. We\nconducted a formal security verification of our scheme using ProVerif and\nassessed its performance through a series of evaluations. The results\ndemonstrate that our scheme is secure and effectively manages the large-scale\ndistribution and updating of unique device keys. Additionally, it achieves\nsignificantly lower update times and data transfer volumes compared to other\nschemes.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11564v1"}}, {"model": "App.cryptographyarticle", "pk": 83, "fields": {"title": "Android App Feature Extraction: A review of approaches for malware and app similarity detection", "date": "2024-12-16", "authors": "Simon Torka, Sahin Albayrak", "content": "This paper reviews work published between 2002 and 2022 in the fields of\nAndroid malware, clone, and similarity detection. It examines the data sources,\ntools, and features used in existing research and identifies the need for a\ncomprehensive, cross-domain dataset to facilitate interdisciplinary\ncollaboration and the exploitation of synergies between different research\nareas. Furthermore, it shows that many research papers do not publish the\ndataset or a description of how it was created, making it difficult to\nreproduce or compare the results. The paper highlights the necessity for a\ndataset that is accessible, well-documented, and suitable for a range of\napplications. Guidelines are provided for this purpose, along with a schematic\nmethod for creating the dataset.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11539v1"}}, {"model": "App.cryptographyarticle", "pk": 84, "fields": {"title": "WFCAT: Augmenting Website Fingerprinting with Channel-wise Attention on Timing Features", "date": "2024-12-16", "authors": "Jiajun Gong, Wei Cai, Siyuan Liang, Zhong Guan, Tao Wang, Ee-Chien Chang", "content": "Website Fingerprinting (WF) aims to deanonymize users on the Tor network by\nanalyzing encrypted network traffic. Recent deep-learning-based attacks show\nhigh accuracy on undefended traces. However, they struggle against modern\ndefenses that use tactics like injecting dummy packets and delaying real\npackets, which significantly degrade classification performance. Our analysis\nreveals that current attacks inadequately leverage the timing information\ninherent in traffic traces, which persists as a source of leakage even under\nrobust defenses. Addressing this shortfall, we introduce a novel feature\nrepresentation named the Inter-Arrival Time (IAT) histogram, which quantifies\nthe frequencies of packet inter-arrival times across predetermined time slots.\nComplementing this feature, we propose a new CNN-based attack, WFCAT, enhanced\nwith two innovative architectural blocks designed to optimally extract and\nutilize timing information. Our approach uses kernels of varying sizes to\ncapture multi-scale features, which are then integrated using a weighted sum\nacross all feature channels to enhance the model's efficacy in identifying\ntemporal patterns. Our experiments validate that WFCAT substantially\noutperforms existing methods on defended traces in both closed- and open-world\nscenarios. Notably, WFCAT achieves over 59% accuracy against Surakav, a\nrecently developed robust defense, marking an improvement of over 28% and 48%\nagainst the state-of-the-art attacks RF and Tik-Tok, respectively, in the\nclosed-world scenario.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11487v1"}}, {"model": "App.cryptographyarticle", "pk": 85, "fields": {"title": "Noise-Resilient Homomorphic Encryption: A Framework for Secure Data Processing in Health care Domain", "date": "2024-12-16", "authors": "B. Shuriya, S. Vimal Kumar, K. Bagyalakshmi", "content": "In this paper, we introduce the Fully Homomorphic Integrity Model (HIM), a\nnovel approach designed to enhance security, efficiency, and reliability in\nencrypted data processing, primarily within the health care industry. HIM\naddresses the key challenges that noise accumulation, computational overheads,\nand data integrity pose during homomorphic operations. Our contribution of HIM:\nadvances in noise management through the rational number adjustment; key\ngeneration based on personalized prime numbers; and time complexity analysis\ndetails for key operations. In HIM, some additional mechanisms were introduced,\nincluding robust mechanisms of decryption. Indeed, the decryption mechanism\nensures that the data recovered upon doing complex homomorphic computation will\nbe valid and reliable. The healthcare id model is tested, and it supports\nreal-time processing of data with privacy maintained concerning patients. It\nsupports analytics and decision-making processes without any compromise on the\nintegrity of information concerning patients. Output HIM promotes the\nefficiency of encryption to a greater extent as it reduces the encryption time\nup to 35ms and decryption time up to 140ms, which is better when compared to\nother models in the existence. Ciphertext size also becomes the smallest one,\nwhich is 4KB. Our experiments confirm that HIM is indeed a very efficient and\nsecure privacy-preserving solution for healthcare applications", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11474v1"}}, {"model": "App.cryptographyarticle", "pk": 86, "fields": {"title": "Red Pill and Blue Pill: Controllable Website Fingerprinting Defense via Dynamic Backdoor Learning", "date": "2024-12-16", "authors": "Siyuan Liang, Jiajun Gong, Tianmeng Fang, Aishan Liu, Tao Wang, Xianglong Liu, Xiaochun Cao, Dacheng Tao, Chang Ee-Chien", "content": "Website fingerprint (WF) attacks, which covertly monitor user communications\nto identify the web pages they visit, pose a serious threat to user privacy.\nExisting WF defenses attempt to reduce the attacker's accuracy by disrupting\nunique traffic patterns; however, they often suffer from the trade-off between\noverhead and effectiveness, resulting in less usefulness in practice. To\novercome this limitation, we introduce Controllable Website Fingerprint Defense\n(CWFD), a novel defense perspective based on backdoor learning. CWFD exploits\nbackdoor vulnerabilities in neural networks to directly control the attacker's\nmodel by designing trigger patterns based on network traffic. Specifically,\nCWFD injects only incoming packets on the server side into the target web\npage's traffic, keeping overhead low while effectively poisoning the attacker's\nmodel during training. During inference, the defender can influence the\nattacker's model through a 'red pill, blue pill' choice: traces with the\ntrigger (red pill) lead to misclassification as the target web page, while\nnormal traces (blue pill) are classified correctly, achieving directed control\nover the defense outcome. We use the Fast Levenshtein-like distance as the\noptimization objective to compute trigger patterns that can be effectively\nassociated with our target page. Experiments show that CWFD significantly\nreduces RF's accuracy from 99% to 6% with 74% data overhead. In comparison,\nFRONT reduces accuracy to only 97% at similar overhead, while Palette achieves\n32% accuracy with 48% more overhead. We further validate the practicality of\nour method in a real Tor network environment.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11471v1"}}, {"model": "App.cryptographyarticle", "pk": 87, "fields": {"title": "UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion Models", "date": "2024-12-16", "authors": "Yuning Han, Bingyin Zhao, Rui Chu, Feng Luo, Biplab Sikdar, Yingjie Lao", "content": "Recent studies show that diffusion models (DMs) are vulnerable to backdoor\nattacks. Existing backdoor attacks impose unconcealed triggers (e.g., a gray\nbox and eyeglasses) that contain evident patterns, rendering remarkable attack\neffects yet easy detection upon human inspection and defensive algorithms.\nWhile it is possible to improve stealthiness by reducing the strength of the\nbackdoor, doing so can significantly compromise its generality and\neffectiveness. In this paper, we propose UIBDiffusion, the universal\nimperceptible backdoor attack for diffusion models, which allows us to achieve\nsuperior attack and generation performance while evading state-of-the-art\ndefenses. We propose a novel trigger generation approach based on universal\nadversarial perturbations (UAPs) and reveal that such perturbations, which are\ninitially devised for fooling pre-trained discriminative models, can be adapted\nas potent imperceptible backdoor triggers for DMs. We evaluate UIBDiffusion on\nmultiple types of DMs with different kinds of samplers across various datasets\nand targets. Experimental results demonstrate that UIBDiffusion brings three\nadvantages: 1) Universality, the imperceptible trigger is universal (i.e.,\nimage and model agnostic) where a single trigger is effective to any images and\nall diffusion models with different samplers; 2) Utility, it achieves\ncomparable generation quality (e.g., FID) and even better attack success rate\n(i.e., ASR) at low poison rates compared to the prior works; and 3)\nUndetectability, UIBDiffusion is plausible to human perception and can bypass\nElijah and TERD, the SOTA defenses against backdoors for DMs. We will release\nour backdoor triggers and code.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11441v1"}}, {"model": "App.cryptographyarticle", "pk": 88, "fields": {"title": "A Comprehensive Review of Adversarial Attacks on Machine Learning", "date": "2024-12-16", "authors": "Syed Quiser Ahmed, Bharathi Vokkaliga Ganesh, Sathyanarayana Sampath Kumar, Prakhar Mishra, Ravi Anand, Bhanuteja Akurathi", "content": "This research provides a comprehensive overview of adversarial attacks on AI\nand ML models, exploring various attack types, techniques, and their potential\nharms. We also delve into the business implications, mitigation strategies, and\nfuture research directions. To gain practical insights, we employ the\nAdversarial Robustness Toolbox (ART) [1] library to simulate these attacks on\nreal-world use cases, such as self-driving cars. Our goal is to inform\npractitioners and researchers about the challenges and opportunities in\ndefending AI systems against adversarial threats. By providing a comprehensive\ncomparison of different attack methods, we aim to contribute to the development\nof more robust and secure AI systems.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11384v1"}}, {"model": "App.cryptographyarticle", "pk": 89, "fields": {"title": "PSGraph: Differentially Private Streaming Graph Synthesis by Considering Temporal Dynamics", "date": "2024-12-16", "authors": "Quan Yuan, Zhikun Zhang, Linkang Du, Min Chen, Mingyang Sun, Yunjun Gao, Michael Backes, Shibo He, Jiming Chen", "content": "Streaming graphs are ubiquitous in daily life, such as evolving social\nnetworks and dynamic communication systems. Due to the sensitive information\ncontained in the graph, directly sharing the streaming graphs poses significant\nprivacy risks. Differential privacy, offering strict theoretical guarantees,\nhas emerged as a standard approach for private graph data synthesis. However,\nexisting methods predominantly focus on static graph publishing, neglecting the\nintrinsic relationship between adjacent graphs, thereby resulting in limited\nperformance in streaming data publishing scenarios. To address this gap, we\npropose PSGraph, the first differentially private streaming graph synthesis\nframework that integrates temporal dynamics. PSGraph adaptively adjusts the\nprivacy budget allocation mechanism by analyzing the variations in the current\ngraph compared to the previous one for conserving the privacy budget. Moreover,\nPSGraph aggregates information across various timestamps and adopts crucial\npost-processing techniques to enhance the synthetic streaming graphs. We\nconduct extensive experiments on four real-world datasets under five commonly\nused metrics. The experimental results demonstrate the superiority of PSGraph.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11369v1"}}, {"model": "App.cryptographyarticle", "pk": 90, "fields": {"title": "Comprehensive Survey on Adversarial Examples in Cybersecurity: Impacts, Challenges, and Mitigation Strategies", "date": "2024-12-16", "authors": "Li Li", "content": "Deep learning (DL) has significantly transformed cybersecurity, enabling\nadvancements in malware detection, botnet identification, intrusion detection,\nuser authentication, and encrypted traffic analysis. However, the rise of\nadversarial examples (AE) poses a critical challenge to the robustness and\nreliability of DL-based systems. These subtle, crafted perturbations can\ndeceive models, leading to severe consequences like misclassification and\nsystem vulnerabilities. This paper provides a comprehensive review of the\nimpact of AE attacks on key cybersecurity applications, highlighting both their\ntheoretical and practical implications. We systematically examine the methods\nused to generate adversarial examples, their specific effects across various\ndomains, and the inherent trade-offs attackers face between efficacy and\nresource efficiency. Additionally, we explore recent advancements in defense\nmechanisms, including gradient masking, adversarial training, and detection\ntechniques, evaluating their potential to enhance model resilience. By\nsummarizing cutting-edge research, this study aims to bridge the gap between\nadversarial research and practical security applications, offering insights to\nfortify the adoption of DL solutions in cybersecurity.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12217v1"}}, {"model": "App.cryptographyarticle", "pk": 91, "fields": {"title": "Finding a Wolf in Sheep's Clothing: Combating Adversarial Text-To-Image Prompts with Text Summarization", "date": "2024-12-15", "authors": "Portia Cooper, Harshita Narnoli, Mihai Surdeanu", "content": "Text-to-image models are vulnerable to the stepwise \"Divide-and-Conquer\nAttack\" (DACA) that utilize a large language model to obfuscate inappropriate\ncontent in prompts by wrapping sensitive text in a benign narrative. To\nmitigate stepwise DACA attacks, we propose a two-layer method involving text\nsummarization followed by binary classification. We assembled the Adversarial\nText-to-Image Prompt (ATTIP) dataset ($N=940$), which contained DACA-obfuscated\nand non-obfuscated prompts. From the ATTIP dataset, we created two summarized\nversions: one generated by a small encoder model and the other by a large\nlanguage model. Then, we used an encoder classifier and a GPT-4o classifier to\nperform content moderation on the summarized and unsummarized prompts. When\ncompared with a classifier that operated over the unsummarized data, our method\nimproved F1 score performance by 31%. Further, the highest recorded F1 score\nachieved (98%) was produced by the encoder classifier on a summarized ATTIP\nvariant. This study indicates that pre-classification text summarization can\ninoculate content detection models against stepwise DACA obfuscations.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12212v1"}}, {"model": "App.cryptographyarticle", "pk": 92, "fields": {"title": "Provably Secure Robust Image Steganography via Cross-Modal Error Correction", "date": "2024-12-15", "authors": "Yuang Qi, Kejiang Chen, Na Zhao, Zijin Yang, Weiming Zhang", "content": "The rapid development of image generation models has facilitated the\nwidespread dissemination of generated images on social networks, creating\nfavorable conditions for provably secure image steganography. However, existing\nmethods face issues such as low quality of generated images and lack of\nsemantic control in the generation process. To leverage provably secure\nsteganography with more effective and high-performance image generation models,\nand to ensure that stego images can accurately extract secret messages even\nafter being uploaded to social networks and subjected to lossy processing such\nas JPEG compression, we propose a high-quality, provably secure, and robust\nimage steganography method based on state-of-the-art autoregressive (AR) image\ngeneration models using Vector-Quantized (VQ) tokenizers. Additionally, we\nemploy a cross-modal error-correction framework that generates stego text from\nstego images to aid in restoring lossy images, ultimately enabling the\nextraction of secret messages embedded within the images. Extensive experiments\nhave demonstrated that the proposed method provides advantages in stego\nquality, embedding capacity, and robustness, while ensuring provable\nundetectability.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.12206v1"}}, {"model": "App.cryptographyarticle", "pk": 93, "fields": {"title": "Knowledge Migration Framework for Smart Contract Vulnerability Detection", "date": "2024-12-15", "authors": "Luqi Wang, Wenbao Jiang", "content": "As a cornerstone of blockchain technology in the 3.0 era, smart contracts\nplay a pivotal role in the evolution of blockchain systems. In order to address\nthe limitations of existing smart contract vulnerability detection models with\nregard to their generalisation capability, an AF-STip smart contract\nvulnerability detection framework incorporating efficient knowledge migration\nis proposed. AF-STip employs the teacher network as the main model and migrates\nthe knowledge processed by the smart contract to the student model using a\ndata-free knowledge distillation method. The student model utilises this\nknowledge to enhance its vulnerability detection capabilities. The approach\nmarkedly enhances the model's capacity for feature extraction and cross-class\nadaptation, while concurrently reducing computational overhead.In order to\nfurther enhance the extraction of vulnerability features, an adaptive fusion\nmodule is proposed in this paper, which aims to strengthen the interaction and\nfusion of feature information.The experimental results demonstrate that the\nSTip model attains an average F1 value detection score of 91.16% for the four\nvulnerabilities without disclosing the original smart contract data. To\nvalidate the viability of the proposed lightweight migration approach, the\nstudent model is deployed in a migration learning task targeting a novel\nvulnerability type, resulting in an accuracy of 91.02% and an F1 score of\n90.46%. To the best of our knowledge, AF-STip is the inaugural model to apply\ndata-free knowledge migration to smart contract vulnerability detection. While\nmarkedly reducing the computational overhead, the method still demonstrates\nexceptional performance in detecting novel vulnerabilities.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11175v1"}}, {"model": "App.cryptographyarticle", "pk": 94, "fields": {"title": "PGD-Imp: Rethinking and Unleashing Potential of Classic PGD with Dual Strategies for Imperceptible Adversarial Attacks", "date": "2024-12-15", "authors": "Jin Li, Zitong Yu, Ziqiang He, Z. Jane Wang, Xiangui Kang", "content": "Imperceptible adversarial attacks have recently attracted increasing research\ninterests. Existing methods typically incorporate external modules or loss\nterms other than a simple $l_p$-norm into the attack process to achieve\nimperceptibility, while we argue that such additional designs may not be\nnecessary. In this paper, we rethink the essence of imperceptible attacks and\npropose two simple yet effective strategies to unleash the potential of PGD,\nthe common and classical attack, for imperceptibility from an optimization\nperspective. Specifically, the Dynamic Step Size is introduced to find the\noptimal solution with minimal attack cost towards the decision boundary of the\nattacked model, and the Adaptive Early Stop strategy is adopted to reduce the\nredundant strength of adversarial perturbations to the minimum level. The\nproposed PGD-Imperceptible (PGD-Imp) attack achieves state-of-the-art results\nin imperceptible adversarial attacks for both untargeted and targeted\nscenarios. When performing untargeted attacks against ResNet-50, PGD-Imp\nattains 100$\\%$ (+0.3$\\%$) ASR, 0.89 (-1.76) $l_2$ distance, and 52.93 (+9.2)\nPSNR with 57s (-371s) running time, significantly outperforming existing\nmethods.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11168v1"}}, {"model": "App.cryptographyarticle", "pk": 95, "fields": {"title": "SpearBot: Leveraging Large Language Models in a Generative-Critique Framework for Spear-Phishing Email Generation", "date": "2024-12-15", "authors": "Qinglin Qi, Yun Luo, Yijia Xu, Wenbo Guo, Yong Fang", "content": "Large Language Models (LLMs) are increasingly capable, aiding in tasks such\nas content generation, yet they also pose risks, particularly in generating\nharmful spear-phishing emails. These emails, crafted to entice clicks on\nmalicious URLs, threaten personal information security. This paper proposes an\nadversarial framework, SpearBot, which utilizes LLMs to generate spear-phishing\nemails with various phishing strategies. Through specifically crafted jailbreak\nprompts, SpearBot circumvents security policies and introduces other LLM\ninstances as critics. When a phishing email is identified by the critic,\nSpearBot refines the generated email based on the critique feedback until it\ncan no longer be recognized as phishing, thereby enhancing its deceptive\nquality. To evaluate the effectiveness of SpearBot, we implement various\nmachine-based defenders and assess how well the phishing emails generated could\ndeceive them. Results show these emails often evade detection to a large\nextent, underscoring their deceptive quality. Additionally, human evaluations\nof the emails' readability and deception are conducted through questionnaires,\nconfirming their convincing nature and the significant potential harm of the\ngenerated phishing emails.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11109v1"}}, {"model": "App.cryptographyarticle", "pk": 96, "fields": {"title": "Learning Robust and Privacy-Preserving Representations via Information Theory", "date": "2024-12-15", "authors": "Binghui Zhang, Sayedeh Leila Noorbakhsh, Yun Dong, Yuan Hong, Binghui Wang", "content": "Machine learning models are vulnerable to both security attacks (e.g.,\nadversarial examples) and privacy attacks (e.g., private attribute inference).\nWe take the first step to mitigate both the security and privacy attacks, and\nmaintain task utility as well. Particularly, we propose an\ninformation-theoretic framework to achieve the goals through the lens of\nrepresentation learning, i.e., learning representations that are robust to both\nadversarial examples and attribute inference adversaries. We also derive novel\ntheoretical results under our framework, e.g., the inherent trade-off between\nadversarial robustness/utility and attribute privacy, and guaranteed attribute\nprivacy leakage against attribute inference adversaries.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11066v1"}}, {"model": "App.cryptographyarticle", "pk": 97, "fields": {"title": "Semantic Steganography: A Framework for Robust and High-Capacity Information Hiding using Large Language Models", "date": "2024-12-15", "authors": "Minhao Bai, Jinshuai Yang, Kaiyi Pang, Yongfeng Huang, Yue Gao", "content": "In the era of Large Language Models (LLMs), generative linguistic\nsteganography has become a prevalent technique for hiding information within\nmodel-generated texts. However, traditional steganography methods struggle to\neffectively align steganographic texts with original model-generated texts due\nto the lower entropy of the predicted probability distribution of LLMs. This\nresults in a decrease in embedding capacity and poses challenges for decoding\nstegos in real-world communication channels. To address these challenges, we\npropose a semantic steganography framework based on LLMs, which construct a\nsemantic space and map secret messages onto this space using ontology-entity\ntrees. This framework offers robustness and reliability for transmission in\ncomplex channels, as well as resistance to text rendering and word blocking.\nAdditionally, the stegos generated by our framework are indistinguishable from\nthe covers and achieve a higher embedding capacity compared to state-of-the-art\nsteganography methods, while producing higher quality stegos.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.11043v1"}}, {"model": "App.cryptographyarticle", "pk": 98, "fields": {"title": "Serial Scammers and Attack of the Clones: How Scammers Coordinate Multiple Rug Pulls on Decentralized Exchanges", "date": "2024-12-14", "authors": "Phuong Duy Huynh, Son Hoang Dau, Hong Yen Tran, Nick Huppert, Hoonie Sun, Joshua Cervenjak, Xiaodong Li, Emanuele Viterbo", "content": "We explored in this work the ubiquitous phenomenon of serial scammers, who\ndeploy thousands of addresses to conduct a series of similar Rug Pulls on\npopular decentralized exchanges (DEXs). We first constructed a list of about\n384,000 scammer addresses behind all 1-day Rug Pulls on the two most popular\nDEXs, Uniswap (Ethereum) and Pancakeswap (BSC), and identified many distinctive\nscam patterns including star-shaped, chain-shaped, and majority-flow scam\nclusters. We then proposed an algorithm to build a complete scam network from\ngiven scammer addresses, which consists of not only scammer addresses but also\nsupporting addresses including depositors, withdrawers, transferrers,\ncoordinators, and most importantly, wash traders. We note that profit\nestimations in existing works on Rug Pulls failed to capture the cost of wash\ntrading, leading to inflated figures. Knowing who the wash traders are, we\nestablished a more accurate estimate for the true profit of individual scam\npools as well as of the entire (serial) scam network by taking into account the\nwash-trading expenses.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.10993v1"}}, {"model": "App.cryptographyarticle", "pk": 99, "fields": {"title": "Labeling NIDS Rules with MITRE ATT&CK Techniques: Machine Learning vs. Large Language Models", "date": "2024-12-14", "authors": "Nir Daniel, Florian Klaus Kaiser, Shay Giladi, Sapir Sharabi, Raz Moyal, Shalev Shpolyansky, Andres Murillo, Aviad Elyashar, Rami Puzis", "content": "Analysts in Security Operations Centers (SOCs) are often occupied with\ntime-consuming investigations of alerts from Network Intrusion Detection\nSystems (NIDS). Many NIDS rules lack clear explanations and associations with\nattack techniques, complicating the alert triage and the generation of attack\nhypotheses. Large Language Models (LLMs) may be a promising technology to\nreduce the alert explainability gap by associating rules with attack\ntechniques. In this paper, we investigate the ability of three prominent LLMs\n(ChatGPT, Claude, and Gemini) to reason about NIDS rules while labeling them\nwith MITRE ATT&CK tactics and techniques. We discuss prompt design and present\nexperiments performed with 973 Snort rules. Our results indicate that while\nLLMs provide explainable, scalable, and efficient initial mappings, traditional\nMachine Learning (ML) models consistently outperform them in accuracy,\nachieving higher precision, recall, and F1-scores. These results highlight the\npotential for hybrid LLM-ML approaches to enhance SOC operations and better\naddress the evolving threat landscape.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.10978v1"}}, {"model": "App.cryptographyarticle", "pk": 100, "fields": {"title": "Automatically Detecting Checked-In Secrets in Android Apps: How Far Are We?", "date": "2024-12-14", "authors": "Kevin Li, Lin Ling, Jinqiu Yang, Lili Wei", "content": "Mobile apps are predominantly integrated with cloud services to benefit from\nenhanced functionalities. Adopting authentication using secrets such as API\nkeys is crucial to ensure secure mobile-cloud interactions. However, developers\noften overlook the proper storage of such secrets, opting to put them directly\ninto their projects. These secrets are checked into the projects and can be\neasily extracted and exploited by malicious adversaries. While many researchers\ninvestigated the issue of checked-in secret in open-source projects, there is a\nnotable research gap concerning checked-in secrets in Android apps deployed on\nplatforms such as Google Play Store. Unlike open-source projects, the lack of\ndirect access to the source code and the presence of obfuscation complicates\nthe checked-in secret detection for Android apps. This motivates us to conduct\nan empirical analysis to measure and compare the performance of different\nchecked-in secret detection tools on Android apps. We first conducted a\nliterature review to find all the checked-in secret detection tools that can be\napplied to Android apps. Then, we evaluate three representative tools on 5,135\nAndroid apps, comparing their performance and analyzing their limitations. Our\nexperiment reveals 2,142 checked-in secrets affecting 2,115 Android apps. We\nalso disclose that the current checked-in secret detection techniques suffer\nfrom key limitations. All of the evaluated tools can miss a significant number\nof checked-in secrets in Android apps. Nevertheless, we observed that the tools\nare complimentary, suggesting the possibility of developing a more effective\nchecked-in secret detection tool by combining their insights. Additionally, we\npropose that analyzing string groups within methods containing checked-in\nsecrets may provide a more effective strategy to overcome obfuscation\nchallenges.", "keywords": "cryptography", "url": "http://arxiv.org/abs/2412.10922v1"}}]